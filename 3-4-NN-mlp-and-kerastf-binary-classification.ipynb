{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING SKLEARN NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "reg = MLPClassifier(hidden_layer_sizes=(2,3),\n",
    "                    verbose=2,\n",
    "                    activation=\"relu\" ,\n",
    "                    batch_size=40,\n",
    "                    random_state=1, \n",
    "                    max_iter=2000, \n",
    "                    learning_rate_init=0.03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ON RANDOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[22,33],\n",
    "           [44,55]]\n",
    "\n",
    "y_train = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.82481770\n",
      "Iteration 2, loss = 0.81072171\n",
      "Iteration 3, loss = 0.79776556\n",
      "Iteration 4, loss = 0.78591496\n",
      "Iteration 5, loss = 0.77512384\n",
      "Iteration 6, loss = 0.76533503\n",
      "Iteration 7, loss = 0.75648167\n",
      "Iteration 8, loss = 0.74848957\n",
      "Iteration 9, loss = 0.74128057\n",
      "Iteration 10, loss = 0.73477665\n",
      "Iteration 11, loss = 0.72890442\n",
      "Iteration 12, loss = 0.72359893\n",
      "Iteration 13, loss = 0.71880638\n",
      "Iteration 14, loss = 0.71448509\n",
      "Iteration 15, loss = 0.71005126\n",
      "Iteration 16, loss = 0.69956078\n",
      "Iteration 17, loss = 0.68669849\n",
      "Iteration 18, loss = 0.66973401\n",
      "Iteration 19, loss = 0.64899605\n",
      "Iteration 20, loss = 0.62857226\n",
      "Iteration 21, loss = 0.61767667\n",
      "Iteration 22, loss = 0.62753616\n",
      "Iteration 23, loss = 0.64651495\n",
      "Iteration 24, loss = 0.64697809\n",
      "Iteration 25, loss = 0.63232672\n",
      "Iteration 26, loss = 0.61643039\n",
      "Iteration 27, loss = 0.60760683\n",
      "Iteration 28, loss = 0.60614063\n",
      "Iteration 29, loss = 0.60802059\n",
      "Iteration 30, loss = 0.60942583\n",
      "Iteration 31, loss = 0.60823544\n",
      "Iteration 32, loss = 0.60367383\n",
      "Iteration 33, loss = 0.59588074\n",
      "Iteration 34, loss = 0.58585924\n",
      "Iteration 35, loss = 0.57553890\n",
      "Iteration 36, loss = 0.56736003\n",
      "Iteration 37, loss = 0.56274436\n",
      "Iteration 38, loss = 0.56006909\n",
      "Iteration 39, loss = 0.55523115\n",
      "Iteration 40, loss = 0.54586124\n",
      "Iteration 41, loss = 0.53378189\n",
      "Iteration 42, loss = 0.52266527\n",
      "Iteration 43, loss = 0.51436458\n",
      "Iteration 44, loss = 0.50742877\n",
      "Iteration 45, loss = 0.49850499\n",
      "Iteration 46, loss = 0.48532673\n",
      "Iteration 47, loss = 0.46895676\n",
      "Iteration 48, loss = 0.45290727\n",
      "Iteration 49, loss = 0.43848650\n",
      "Iteration 50, loss = 0.42182060\n",
      "Iteration 51, loss = 0.40009189\n",
      "Iteration 52, loss = 0.37756797\n",
      "Iteration 53, loss = 0.35810815\n",
      "Iteration 54, loss = 0.33412530\n",
      "Iteration 55, loss = 0.30517067\n",
      "Iteration 56, loss = 0.28073992\n",
      "Iteration 57, loss = 0.25327360\n",
      "Iteration 58, loss = 0.24557065\n",
      "Iteration 59, loss = 0.21365956\n",
      "Iteration 60, loss = 0.19366877\n",
      "Iteration 61, loss = 0.20179384\n",
      "Iteration 62, loss = 0.18935388\n",
      "Iteration 63, loss = 0.16738017\n",
      "Iteration 64, loss = 0.18009649\n",
      "Iteration 65, loss = 0.15271117\n",
      "Iteration 66, loss = 0.15127405\n",
      "Iteration 67, loss = 0.14849656\n",
      "Iteration 68, loss = 0.14417533\n",
      "Iteration 69, loss = 0.13872283\n",
      "Iteration 70, loss = 0.13340166\n",
      "Iteration 71, loss = 0.13287417\n",
      "Iteration 72, loss = 0.12923599\n",
      "Iteration 73, loss = 0.13536980\n",
      "Iteration 74, loss = 0.14521452\n",
      "Iteration 75, loss = 0.14026298\n",
      "Iteration 76, loss = 0.12573179\n",
      "Iteration 77, loss = 0.11639298\n",
      "Iteration 78, loss = 0.11154653\n",
      "Iteration 79, loss = 0.10844978\n",
      "Iteration 80, loss = 0.10594372\n",
      "Iteration 81, loss = 0.15442436\n",
      "Iteration 82, loss = 0.10159136\n",
      "Iteration 83, loss = 0.10004861\n",
      "Iteration 84, loss = 0.09997831\n",
      "Iteration 85, loss = 0.10316968\n",
      "Iteration 86, loss = 0.10909066\n",
      "Iteration 87, loss = 0.10869712\n",
      "Iteration 88, loss = 0.09992467\n",
      "Iteration 89, loss = 0.09218552\n",
      "Iteration 90, loss = 0.08770965\n",
      "Iteration 91, loss = 0.08498666\n",
      "Iteration 92, loss = 0.08297631\n",
      "Iteration 93, loss = 0.08125452\n",
      "Iteration 94, loss = 0.07966605\n",
      "Iteration 95, loss = 0.07815378\n",
      "Iteration 96, loss = 0.09238166\n",
      "Iteration 97, loss = 0.07530337\n",
      "Iteration 98, loss = 0.07402772\n",
      "Iteration 99, loss = 0.07298796\n",
      "Iteration 100, loss = 0.07242553\n",
      "Iteration 101, loss = 0.07268556\n",
      "Iteration 102, loss = 0.07383869\n",
      "Iteration 103, loss = 0.07480491\n",
      "Iteration 104, loss = 0.07372283\n",
      "Iteration 105, loss = 0.07066118\n",
      "Iteration 106, loss = 0.06740084\n",
      "Iteration 107, loss = 0.06487873\n",
      "Iteration 108, loss = 0.06304644\n",
      "Iteration 109, loss = 0.06163726\n",
      "Iteration 110, loss = 0.06045917\n",
      "Iteration 111, loss = 0.05940779\n",
      "Iteration 112, loss = 0.05843090\n",
      "Iteration 113, loss = 0.05750265\n",
      "Iteration 114, loss = 0.05660994\n",
      "Iteration 115, loss = 0.05574584\n",
      "Iteration 116, loss = 0.05490648\n",
      "Iteration 117, loss = 0.05408949\n",
      "Iteration 118, loss = 0.05329336\n",
      "Iteration 119, loss = 0.05251696\n",
      "Iteration 120, loss = 0.05711276\n",
      "Iteration 121, loss = 0.05102957\n",
      "Iteration 122, loss = 0.05034862\n",
      "Iteration 123, loss = 0.04976481\n",
      "Iteration 124, loss = 0.04937821\n",
      "Iteration 125, loss = 0.04935468\n",
      "Iteration 126, loss = 0.04985616\n",
      "Iteration 127, loss = 0.05074996\n",
      "Iteration 128, loss = 0.05125863\n",
      "Iteration 129, loss = 0.05050582\n",
      "Iteration 130, loss = 0.04871972\n",
      "Iteration 131, loss = 0.04682248\n",
      "Iteration 132, loss = 0.04530922\n",
      "Iteration 133, loss = 0.04419062\n",
      "Iteration 134, loss = 0.04333669\n",
      "Iteration 135, loss = 0.04263770\n",
      "Iteration 136, loss = 0.04202699\n",
      "Iteration 137, loss = 0.04146808\n",
      "Iteration 138, loss = 0.04094161\n",
      "Iteration 139, loss = 0.04043722\n",
      "Iteration 140, loss = 0.03994923\n",
      "Iteration 141, loss = 0.03947446\n",
      "Iteration 142, loss = 0.03901100\n",
      "Iteration 143, loss = 0.03855769\n",
      "Iteration 144, loss = 0.03811375\n",
      "Iteration 145, loss = 0.03767864\n",
      "Iteration 146, loss = 0.03725197\n",
      "Iteration 147, loss = 0.03683343\n",
      "Iteration 148, loss = 0.03642274\n",
      "Iteration 149, loss = 0.03601967\n",
      "Iteration 150, loss = 0.03601774\n",
      "Iteration 151, loss = 0.03524241\n",
      "Iteration 152, loss = 0.03487833\n",
      "Iteration 153, loss = 0.03454334\n",
      "Iteration 154, loss = 0.03425746\n",
      "Iteration 155, loss = 0.03405053\n",
      "Iteration 156, loss = 0.03395765\n",
      "Iteration 157, loss = 0.03400028\n",
      "Iteration 158, loss = 0.03414749\n",
      "Iteration 159, loss = 0.03427829\n",
      "Iteration 160, loss = 0.03421139\n",
      "Iteration 161, loss = 0.03383427\n",
      "Iteration 162, loss = 0.03320250\n",
      "Iteration 163, loss = 0.03247521\n",
      "Iteration 164, loss = 0.03178482\n",
      "Iteration 165, loss = 0.03118657\n",
      "Iteration 166, loss = 0.03068152\n",
      "Iteration 167, loss = 0.03025031\n",
      "Iteration 168, loss = 0.02987197\n",
      "Iteration 169, loss = 0.02953006\n",
      "Iteration 170, loss = 0.02921322\n",
      "Iteration 171, loss = 0.02891397\n",
      "Iteration 172, loss = 0.02862752\n",
      "Iteration 173, loss = 0.02835077\n",
      "Iteration 174, loss = 0.02808172\n",
      "Iteration 175, loss = 0.02781907\n",
      "Iteration 176, loss = 0.02756194\n",
      "Iteration 177, loss = 0.02730971\n",
      "Iteration 178, loss = 0.02706196\n",
      "Iteration 179, loss = 0.02681839\n",
      "Iteration 180, loss = 0.02657876\n",
      "Iteration 181, loss = 0.02634288\n",
      "Iteration 182, loss = 0.02611063\n",
      "Iteration 183, loss = 0.02588187\n",
      "Iteration 184, loss = 0.02565651\n",
      "Iteration 185, loss = 0.02543446\n",
      "Iteration 186, loss = 0.02521563\n",
      "Iteration 187, loss = 0.02499995\n",
      "Iteration 188, loss = 0.02478736\n",
      "Iteration 189, loss = 0.02457779\n",
      "Iteration 190, loss = 0.02437118\n",
      "Iteration 191, loss = 0.02416747\n",
      "Iteration 192, loss = 0.02396660\n",
      "Iteration 193, loss = 0.02376853\n",
      "Iteration 194, loss = 0.02357318\n",
      "Iteration 195, loss = 0.02338053\n",
      "Iteration 196, loss = 0.02319051\n",
      "Iteration 197, loss = 0.02300307\n",
      "Iteration 198, loss = 0.02281817\n",
      "Iteration 199, loss = 0.02263576\n",
      "Iteration 200, loss = 0.02245580\n",
      "Iteration 201, loss = 0.02227824\n",
      "Iteration 202, loss = 0.02210303\n",
      "Iteration 203, loss = 0.02193014\n",
      "Iteration 204, loss = 0.02175952\n",
      "Iteration 205, loss = 0.02159112\n",
      "Iteration 206, loss = 0.02142492\n",
      "Iteration 207, loss = 0.02126087\n",
      "Iteration 208, loss = 0.02109893\n",
      "Iteration 209, loss = 0.02093906\n",
      "Iteration 210, loss = 0.02078123\n",
      "Iteration 211, loss = 0.02062541\n",
      "Iteration 212, loss = 0.02047154\n",
      "Iteration 213, loss = 0.02031961\n",
      "Iteration 214, loss = 0.02016958\n",
      "Iteration 215, loss = 0.02002141\n",
      "Iteration 216, loss = 0.01987507\n",
      "Iteration 217, loss = 0.01973053\n",
      "Iteration 218, loss = 0.01958775\n",
      "Iteration 219, loss = 0.01944672\n",
      "Iteration 220, loss = 0.01930740\n",
      "Iteration 221, loss = 0.01916975\n",
      "Iteration 222, loss = 0.01903375\n",
      "Iteration 223, loss = 0.01889938\n",
      "Iteration 224, loss = 0.01876660\n",
      "Iteration 225, loss = 0.01863539\n",
      "Iteration 226, loss = 0.01850573\n",
      "Iteration 227, loss = 0.01837758\n",
      "Iteration 228, loss = 0.01825092\n",
      "Iteration 229, loss = 0.01812573\n",
      "Iteration 230, loss = 0.01800198\n",
      "Iteration 231, loss = 0.01787965\n",
      "Iteration 232, loss = 0.01775872\n",
      "Iteration 233, loss = 0.01763916\n",
      "Iteration 234, loss = 0.01752095\n",
      "Iteration 235, loss = 0.01740407\n",
      "Iteration 236, loss = 0.01728851\n",
      "Iteration 237, loss = 0.01717423\n",
      "Iteration 238, loss = 0.01706122\n",
      "Iteration 239, loss = 0.01694945\n",
      "Iteration 240, loss = 0.01683892\n",
      "Iteration 241, loss = 0.01672959\n",
      "Iteration 242, loss = 0.01662146\n",
      "Iteration 243, loss = 0.01651449\n",
      "Iteration 244, loss = 0.01640868\n",
      "Iteration 245, loss = 0.01630401\n",
      "Iteration 246, loss = 0.01620046\n",
      "Iteration 247, loss = 0.01609801\n",
      "Iteration 248, loss = 0.01599664\n",
      "Iteration 249, loss = 0.01589635\n",
      "Iteration 250, loss = 0.01579710\n",
      "Iteration 251, loss = 0.01569890\n",
      "Iteration 252, loss = 0.01560172\n",
      "Iteration 253, loss = 0.01550554\n",
      "Iteration 254, loss = 0.01541036\n",
      "Iteration 255, loss = 0.01531615\n",
      "Iteration 256, loss = 0.01522291\n",
      "Iteration 257, loss = 0.01513062\n",
      "Iteration 258, loss = 0.01503926\n",
      "Iteration 259, loss = 0.01494883\n",
      "Iteration 260, loss = 0.01485931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=40, hidden_layer_sizes=(2, 3), learning_rate_init=0.03,\n",
       "              max_iter=2000, random_state=1, verbose=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =[[22,33],\n",
    "           [44,55]]\n",
    "\n",
    "y_test= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "y_pred=reg.predict(X_test)\n",
    "y_pred\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.71121293e-01, 2.88787074e-02],\n",
       "       [9.97382736e-06, 9.99990026e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred_prob=reg.predict_proba(X_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING KERAS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ON RANDOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =[[22.,33.],\n",
    "           [44.,55.]]\n",
    "\n",
    "X_train =[[22.,33.],\n",
    "           [44.,55.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22.0, 33.0], [44.0, 55.0]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[0, 1]\n",
    "y_test=[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our vectorized labels\n",
    "\n",
    "y_train = np.asarray(y_train).astype('float32')#.reshape((-1,1))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "y_test = np.asarray(y_test).astype('float32')#.reshape((-1,1))\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Set the input shape\n",
    "\n",
    "input_shape = (2,)\n",
    "\n",
    "print(f'Feature shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(3, input_shape=input_shape, activation='relu'))\n",
    "\n",
    "model.add(Dense(3, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you're doing a binary classification, make sure that that your last Dense layer has just (None, 1) in its shape, not None, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "\n",
    "#metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22.0, 33.0], [44.0, 55.0]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 600ms/step - loss: 0.9718 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9665 - accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9620 - accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9583 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9553 - accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9526 - accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9501 - accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9475 - accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9448 - accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9420 - accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9392 - accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9363 - accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9333 - accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9304 - accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9276 - accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9247 - accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9220 - accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9193 - accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9166 - accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9140 - accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9113 - accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9087 - accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9060 - accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9033 - accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9006 - accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8980 - accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8953 - accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8927 - accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8900 - accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8874 - accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8849 - accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8823 - accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8798 - accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8772 - accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8747 - accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8722 - accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8697 - accuracy: 0.5000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8672 - accuracy: 0.5000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8648 - accuracy: 0.5000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8623 - accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8599 - accuracy: 0.5000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8575 - accuracy: 0.5000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8551 - accuracy: 0.5000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8527 - accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8504 - accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8480 - accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8457 - accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8434 - accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8411 - accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8388 - accuracy: 0.5000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8365 - accuracy: 0.5000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8343 - accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8320 - accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8298 - accuracy: 0.5000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8276 - accuracy: 0.5000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8254 - accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8232 - accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8211 - accuracy: 0.5000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8189 - accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8168 - accuracy: 0.5000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8147 - accuracy: 0.5000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8126 - accuracy: 0.5000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8105 - accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8085 - accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8064 - accuracy: 0.5000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8044 - accuracy: 0.5000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8024 - accuracy: 0.5000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8004 - accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7984 - accuracy: 0.5000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7965 - accuracy: 0.5000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7945 - accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7926 - accuracy: 0.5000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7907 - accuracy: 0.5000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7888 - accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7869 - accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7851 - accuracy: 0.5000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7832 - accuracy: 0.5000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7814 - accuracy: 0.5000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7796 - accuracy: 0.5000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7778 - accuracy: 0.5000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7760 - accuracy: 0.5000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7742 - accuracy: 0.5000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7725 - accuracy: 0.5000\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7707 - accuracy: 0.5000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7690 - accuracy: 0.5000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7673 - accuracy: 0.5000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7656 - accuracy: 0.5000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7639 - accuracy: 0.5000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7623 - accuracy: 0.5000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7606 - accuracy: 0.5000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7590 - accuracy: 0.5000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7574 - accuracy: 0.5000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7558 - accuracy: 0.5000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7542 - accuracy: 0.5000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7527 - accuracy: 0.5000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7511 - accuracy: 0.5000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7496 - accuracy: 0.5000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7481 - accuracy: 0.5000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7466 - accuracy: 0.5000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7451 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c7bbfbf130>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#and start training\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=100, \n",
    "          batch_size=20, \n",
    "          verbose=1\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: Data cardinality is ambiguous:\n",
    "  x sizes: 691\n",
    "  y sizes: 2\n",
    "Make sure all arrays contain the same number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'float\\'>\"})'}), <class 'numpy.ndarray'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   \n",
    "   \n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "# load the dataset\n",
    "\n",
    "dataset = loadtxt('diabetes.csv', delimiter=',')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(dataset).isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset).duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = dataset[:,0:8]\n",
    "\n",
    "y = dataset[:,8]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 7.900e+01, 6.000e+01, 4.200e+01, 4.800e+01, 4.350e+01,\n",
       "        6.780e-01, 2.300e+01],\n",
       "       [0.000e+00, 8.600e+01, 6.800e+01, 3.200e+01, 0.000e+00, 3.580e+01,\n",
       "        2.380e-01, 2.500e+01],\n",
       "       [5.000e+00, 1.620e+02, 1.040e+02, 0.000e+00, 0.000e+00, 3.770e+01,\n",
       "        1.510e-01, 5.200e+01],\n",
       "       [8.000e+00, 6.500e+01, 7.200e+01, 2.300e+01, 0.000e+00, 3.200e+01,\n",
       "        6.000e-01, 4.200e+01],\n",
       "       [9.000e+00, 1.230e+02, 7.000e+01, 4.400e+01, 9.400e+01, 3.310e+01,\n",
       "        3.740e-01, 4.000e+01],\n",
       "       [8.000e+00, 1.540e+02, 7.800e+01, 3.200e+01, 0.000e+00, 3.240e+01,\n",
       "        4.430e-01, 4.500e+01],\n",
       "       [1.700e+01, 1.630e+02, 7.200e+01, 4.100e+01, 1.140e+02, 4.090e+01,\n",
       "        8.170e-01, 4.700e+01],\n",
       "       [0.000e+00, 1.040e+02, 6.400e+01, 3.700e+01, 6.400e+01, 3.360e+01,\n",
       "        5.100e-01, 2.200e+01],\n",
       "       [1.000e+00, 1.960e+02, 7.600e+01, 3.600e+01, 2.490e+02, 3.650e+01,\n",
       "        8.750e-01, 2.900e+01],\n",
       "       [1.000e+00, 1.170e+02, 6.000e+01, 2.300e+01, 1.060e+02, 3.380e+01,\n",
       "        4.660e-01, 2.700e+01],\n",
       "       [4.000e+00, 8.400e+01, 9.000e+01, 2.300e+01, 5.600e+01, 3.950e+01,\n",
       "        1.590e-01, 2.500e+01],\n",
       "       [8.000e+00, 1.790e+02, 7.200e+01, 4.200e+01, 1.300e+02, 3.270e+01,\n",
       "        7.190e-01, 3.600e+01],\n",
       "       [1.000e+00, 1.200e+02, 8.000e+01, 4.800e+01, 2.000e+02, 3.890e+01,\n",
       "        1.162e+00, 4.100e+01],\n",
       "       [6.000e+00, 1.050e+02, 8.000e+01, 2.800e+01, 0.000e+00, 3.250e+01,\n",
       "        8.780e-01, 2.600e+01],\n",
       "       [3.000e+00, 1.800e+02, 6.400e+01, 2.500e+01, 7.000e+01, 3.400e+01,\n",
       "        2.710e-01, 2.600e+01],\n",
       "       [1.200e+01, 8.800e+01, 7.400e+01, 4.000e+01, 5.400e+01, 3.530e+01,\n",
       "        3.780e-01, 4.800e+01],\n",
       "       [2.000e+00, 9.900e+01, 5.200e+01, 1.500e+01, 9.400e+01, 2.460e+01,\n",
       "        6.370e-01, 2.100e+01],\n",
       "       [1.000e+00, 1.150e+02, 7.000e+01, 3.000e+01, 9.600e+01, 3.460e+01,\n",
       "        5.290e-01, 3.200e+01],\n",
       "       [0.000e+00, 1.400e+02, 6.500e+01, 2.600e+01, 1.300e+02, 4.260e+01,\n",
       "        4.310e-01, 2.400e+01],\n",
       "       [1.100e+01, 8.500e+01, 7.400e+01, 0.000e+00, 0.000e+00, 3.010e+01,\n",
       "        3.000e-01, 3.500e+01],\n",
       "       [4.000e+00, 1.290e+02, 8.600e+01, 2.000e+01, 2.700e+02, 3.510e+01,\n",
       "        2.310e-01, 2.300e+01],\n",
       "       [1.000e+00, 7.100e+01, 4.800e+01, 1.800e+01, 7.600e+01, 2.040e+01,\n",
       "        3.230e-01, 2.200e+01],\n",
       "       [5.000e+00, 1.440e+02, 8.200e+01, 2.600e+01, 2.850e+02, 3.200e+01,\n",
       "        4.520e-01, 5.800e+01],\n",
       "       [5.000e+00, 1.870e+02, 7.600e+01, 2.700e+01, 2.070e+02, 4.360e+01,\n",
       "        1.034e+00, 5.300e+01],\n",
       "       [3.000e+00, 9.000e+01, 7.800e+01, 0.000e+00, 0.000e+00, 4.270e+01,\n",
       "        5.590e-01, 2.100e+01],\n",
       "       [1.300e+01, 1.260e+02, 9.000e+01, 0.000e+00, 0.000e+00, 4.340e+01,\n",
       "        5.830e-01, 4.200e+01],\n",
       "       [1.000e+00, 1.390e+02, 6.200e+01, 4.100e+01, 4.800e+02, 4.070e+01,\n",
       "        5.360e-01, 2.100e+01],\n",
       "       [5.000e+00, 1.260e+02, 7.800e+01, 2.700e+01, 2.200e+01, 2.960e+01,\n",
       "        4.390e-01, 4.000e+01],\n",
       "       [1.000e+00, 8.800e+01, 3.000e+01, 4.200e+01, 9.900e+01, 5.500e+01,\n",
       "        4.960e-01, 2.600e+01],\n",
       "       [4.000e+00, 1.450e+02, 8.200e+01, 1.800e+01, 0.000e+00, 3.250e+01,\n",
       "        2.350e-01, 7.000e+01],\n",
       "       [1.000e+00, 8.000e+01, 7.400e+01, 1.100e+01, 6.000e+01, 3.000e+01,\n",
       "        5.270e-01, 2.200e+01],\n",
       "       [8.000e+00, 1.960e+02, 7.600e+01, 2.900e+01, 2.800e+02, 3.750e+01,\n",
       "        6.050e-01, 5.700e+01],\n",
       "       [1.000e+00, 7.700e+01, 5.600e+01, 3.000e+01, 5.600e+01, 3.330e+01,\n",
       "        1.251e+00, 2.400e+01],\n",
       "       [9.000e+00, 1.340e+02, 7.400e+01, 3.300e+01, 6.000e+01, 2.590e+01,\n",
       "        4.600e-01, 8.100e+01],\n",
       "       [5.000e+00, 8.500e+01, 7.400e+01, 2.200e+01, 0.000e+00, 2.900e+01,\n",
       "        1.224e+00, 3.200e+01],\n",
       "       [5.000e+00, 1.140e+02, 7.400e+01, 0.000e+00, 0.000e+00, 2.490e+01,\n",
       "        7.440e-01, 5.700e+01],\n",
       "       [7.000e+00, 1.870e+02, 5.000e+01, 3.300e+01, 3.920e+02, 3.390e+01,\n",
       "        8.260e-01, 3.400e+01],\n",
       "       [0.000e+00, 1.050e+02, 6.800e+01, 2.200e+01, 0.000e+00, 2.000e+01,\n",
       "        2.360e-01, 2.200e+01],\n",
       "       [2.000e+00, 1.020e+02, 8.600e+01, 3.600e+01, 1.200e+02, 4.550e+01,\n",
       "        1.270e-01, 2.300e+01],\n",
       "       [3.000e+00, 1.210e+02, 5.200e+01, 0.000e+00, 0.000e+00, 3.600e+01,\n",
       "        1.270e-01, 2.500e+01],\n",
       "       [7.000e+00, 1.030e+02, 6.600e+01, 3.200e+01, 0.000e+00, 3.910e+01,\n",
       "        3.440e-01, 3.100e+01],\n",
       "       [6.000e+00, 1.650e+02, 6.800e+01, 2.600e+01, 1.680e+02, 3.360e+01,\n",
       "        6.310e-01, 4.900e+01],\n",
       "       [8.000e+00, 1.000e+02, 7.400e+01, 4.000e+01, 2.150e+02, 3.940e+01,\n",
       "        6.610e-01, 4.300e+01],\n",
       "       [6.000e+00, 1.030e+02, 6.600e+01, 0.000e+00, 0.000e+00, 2.430e+01,\n",
       "        2.490e-01, 2.900e+01],\n",
       "       [6.000e+00, 1.140e+02, 8.800e+01, 0.000e+00, 0.000e+00, 2.780e+01,\n",
       "        2.470e-01, 6.600e+01],\n",
       "       [6.000e+00, 1.900e+02, 9.200e+01, 0.000e+00, 0.000e+00, 3.550e+01,\n",
       "        2.780e-01, 6.600e+01],\n",
       "       [2.000e+00, 1.300e+02, 9.600e+01, 0.000e+00, 0.000e+00, 2.260e+01,\n",
       "        2.680e-01, 2.100e+01],\n",
       "       [4.000e+00, 1.100e+02, 9.200e+01, 0.000e+00, 0.000e+00, 3.760e+01,\n",
       "        1.910e-01, 3.000e+01],\n",
       "       [4.000e+00, 1.710e+02, 7.200e+01, 0.000e+00, 0.000e+00, 4.360e+01,\n",
       "        4.790e-01, 2.600e+01],\n",
       "       [1.000e+01, 1.790e+02, 7.000e+01, 0.000e+00, 0.000e+00, 3.510e+01,\n",
       "        2.000e-01, 3.700e+01],\n",
       "       [1.000e+01, 6.800e+01, 1.060e+02, 2.300e+01, 4.900e+01, 3.550e+01,\n",
       "        2.850e-01, 4.700e+01],\n",
       "       [0.000e+00, 1.040e+02, 7.600e+01, 0.000e+00, 0.000e+00, 1.840e+01,\n",
       "        5.820e-01, 2.700e+01],\n",
       "       [5.000e+00, 1.390e+02, 8.000e+01, 3.500e+01, 1.600e+02, 3.160e+01,\n",
       "        3.610e-01, 2.500e+01],\n",
       "       [0.000e+00, 1.790e+02, 5.000e+01, 3.600e+01, 1.590e+02, 3.780e+01,\n",
       "        4.550e-01, 2.200e+01],\n",
       "       [7.000e+00, 1.330e+02, 8.400e+01, 0.000e+00, 0.000e+00, 4.020e+01,\n",
       "        6.960e-01, 3.700e+01],\n",
       "       [9.000e+00, 1.560e+02, 8.600e+01, 2.800e+01, 1.550e+02, 3.430e+01,\n",
       "        1.189e+00, 4.200e+01],\n",
       "       [3.000e+00, 1.070e+02, 6.200e+01, 1.300e+01, 4.800e+01, 2.290e+01,\n",
       "        6.780e-01, 2.300e+01],\n",
       "       [5.000e+00, 8.800e+01, 7.800e+01, 3.000e+01, 0.000e+00, 2.760e+01,\n",
       "        2.580e-01, 3.700e+01],\n",
       "       [3.000e+00, 1.930e+02, 7.000e+01, 3.100e+01, 0.000e+00, 3.490e+01,\n",
       "        2.410e-01, 2.500e+01],\n",
       "       [6.000e+00, 9.300e+01, 5.000e+01, 3.000e+01, 6.400e+01, 2.870e+01,\n",
       "        3.560e-01, 2.300e+01],\n",
       "       [2.000e+00, 1.120e+02, 6.800e+01, 2.200e+01, 9.400e+01, 3.410e+01,\n",
       "        3.150e-01, 2.600e+01],\n",
       "       [1.000e+00, 1.080e+02, 6.000e+01, 4.600e+01, 1.780e+02, 3.550e+01,\n",
       "        4.150e-01, 2.400e+01],\n",
       "       [3.000e+00, 1.280e+02, 7.200e+01, 2.500e+01, 1.900e+02, 3.240e+01,\n",
       "        5.490e-01, 2.700e+01],\n",
       "       [1.000e+00, 1.240e+02, 6.000e+01, 3.200e+01, 0.000e+00, 3.580e+01,\n",
       "        5.140e-01, 2.100e+01],\n",
       "       [8.000e+00, 1.120e+02, 7.200e+01, 0.000e+00, 0.000e+00, 2.360e+01,\n",
       "        8.400e-01, 5.800e+01],\n",
       "       [2.000e+00, 1.220e+02, 6.000e+01, 1.800e+01, 1.060e+02, 2.980e+01,\n",
       "        7.170e-01, 2.200e+01],\n",
       "       [2.000e+00, 1.150e+02, 6.400e+01, 2.200e+01, 0.000e+00, 3.080e+01,\n",
       "        4.210e-01, 2.100e+01],\n",
       "       [1.000e+00, 8.100e+01, 7.200e+01, 1.800e+01, 4.000e+01, 2.660e+01,\n",
       "        2.830e-01, 2.400e+01],\n",
       "       [0.000e+00, 1.130e+02, 8.000e+01, 1.600e+01, 0.000e+00, 3.100e+01,\n",
       "        8.740e-01, 2.100e+01],\n",
       "       [2.000e+00, 9.000e+01, 8.000e+01, 1.400e+01, 5.500e+01, 2.440e+01,\n",
       "        2.490e-01, 2.400e+01],\n",
       "       [0.000e+00, 7.300e+01, 0.000e+00, 0.000e+00, 0.000e+00, 2.110e+01,\n",
       "        3.420e-01, 2.500e+01],\n",
       "       [1.100e+01, 1.430e+02, 9.400e+01, 3.300e+01, 1.460e+02, 3.660e+01,\n",
       "        2.540e-01, 5.100e+01],\n",
       "       [0.000e+00, 1.370e+02, 6.800e+01, 1.400e+01, 1.480e+02, 2.480e+01,\n",
       "        1.430e-01, 2.100e+01],\n",
       "       [0.000e+00, 1.190e+02, 6.600e+01, 2.700e+01, 0.000e+00, 3.880e+01,\n",
       "        2.590e-01, 2.200e+01],\n",
       "       [1.200e+01, 8.400e+01, 7.200e+01, 3.100e+01, 0.000e+00, 2.970e+01,\n",
       "        2.970e-01, 4.600e+01],\n",
       "       [0.000e+00, 1.270e+02, 8.000e+01, 3.700e+01, 2.100e+02, 3.630e+01,\n",
       "        8.040e-01, 2.300e+01],\n",
       "       [7.000e+00, 1.190e+02, 0.000e+00, 0.000e+00, 0.000e+00, 2.520e+01,\n",
       "        2.090e-01, 3.700e+01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\n",
    "model2.add(Dense(8, activation='relu'))\n",
    "\n",
    "model2.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.0111 - accuracy: 0.6324\n",
      "Epoch 2/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.6953 - accuracy: 0.6295\n",
      "Epoch 3/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.5194 - accuracy: 0.6454\n",
      "Epoch 4/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.8190 - accuracy: 0.6397\n",
      "Epoch 5/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 3.2046 - accuracy: 0.6425\n",
      "Epoch 6/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 2.6686 - accuracy: 0.6208\n",
      "Epoch 7/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.2409 - accuracy: 0.6064\n",
      "Epoch 8/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.8423 - accuracy: 0.6078\n",
      "Epoch 9/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.5649 - accuracy: 0.6237\n",
      "Epoch 10/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.3641 - accuracy: 0.6252\n",
      "Epoch 11/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2266 - accuracy: 0.6353\n",
      "Epoch 12/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.1224 - accuracy: 0.6078\n",
      "Epoch 13/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.0395 - accuracy: 0.6020\n",
      "Epoch 14/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.9715 - accuracy: 0.5991\n",
      "Epoch 15/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.9169 - accuracy: 0.5745\n",
      "Epoch 16/150\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8835 - accuracy: 0.5760\n",
      "Epoch 17/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8503 - accuracy: 0.5861\n",
      "Epoch 18/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8215 - accuracy: 0.5832\n",
      "Epoch 19/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8069 - accuracy: 0.5818\n",
      "Epoch 20/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7961 - accuracy: 0.5832\n",
      "Epoch 21/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7673 - accuracy: 0.5962\n",
      "Epoch 22/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.7572 - accuracy: 0.5861\n",
      "Epoch 23/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7499 - accuracy: 0.5919\n",
      "Epoch 24/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7497 - accuracy: 0.5789\n",
      "Epoch 25/150\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7287 - accuracy: 0.5760\n",
      "Epoch 26/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7242 - accuracy: 0.5832\n",
      "Epoch 27/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.7133 - accuracy: 0.5803\n",
      "Epoch 28/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.7020 - accuracy: 0.5818\n",
      "Epoch 29/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6959 - accuracy: 0.5789\n",
      "Epoch 30/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.5760\n",
      "Epoch 31/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5861\n",
      "Epoch 32/150\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6883 - accuracy: 0.5760\n",
      "Epoch 33/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6789 - accuracy: 0.5832\n",
      "Epoch 34/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6731 - accuracy: 0.5832\n",
      "Epoch 35/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6708 - accuracy: 0.5818\n",
      "Epoch 36/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6668 - accuracy: 0.5818\n",
      "Epoch 37/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6649 - accuracy: 0.5904\n",
      "Epoch 38/150\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6692 - accuracy: 0.5760\n",
      "Epoch 39/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6696 - accuracy: 0.6469\n",
      "Epoch 40/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6893 - accuracy: 0.6498\n",
      "Epoch 41/150\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6838 - accuracy: 0.6570\n",
      "Epoch 42/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6553 - accuracy: 0.6599\n",
      "Epoch 43/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6568 - accuracy: 0.6585\n",
      "Epoch 44/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6596 - accuracy: 0.6527\n",
      "Epoch 45/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6455 - accuracy: 0.6643\n",
      "Epoch 46/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6415 - accuracy: 0.6628\n",
      "Epoch 47/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6450 - accuracy: 0.6599\n",
      "Epoch 48/150\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6400 - accuracy: 0.6686\n",
      "Epoch 49/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6394 - accuracy: 0.6744\n",
      "Epoch 50/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6414 - accuracy: 0.6657\n",
      "Epoch 51/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6344 - accuracy: 0.6643\n",
      "Epoch 52/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6383 - accuracy: 0.6715\n",
      "Epoch 53/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6310 - accuracy: 0.6628\n",
      "Epoch 54/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6744\n",
      "Epoch 55/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6302 - accuracy: 0.6686\n",
      "Epoch 56/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6270 - accuracy: 0.6773\n",
      "Epoch 57/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6298 - accuracy: 0.6671\n",
      "Epoch 58/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6272 - accuracy: 0.6729\n",
      "Epoch 59/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6268 - accuracy: 0.6643\n",
      "Epoch 60/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6234 - accuracy: 0.6773\n",
      "Epoch 61/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6245 - accuracy: 0.6758\n",
      "Epoch 62/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6217 - accuracy: 0.6729\n",
      "Epoch 63/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6300 - accuracy: 0.6715\n",
      "Epoch 64/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6280 - accuracy: 0.6614\n",
      "Epoch 65/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6197 - accuracy: 0.6758\n",
      "Epoch 66/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6215 - accuracy: 0.6744\n",
      "Epoch 67/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6220 - accuracy: 0.6715\n",
      "Epoch 68/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6274 - accuracy: 0.6657\n",
      "Epoch 69/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6271 - accuracy: 0.6729\n",
      "Epoch 70/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6179 - accuracy: 0.6816\n",
      "Epoch 71/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6162 - accuracy: 0.6787\n",
      "Epoch 72/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6136 - accuracy: 0.6802\n",
      "Epoch 73/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.6802\n",
      "Epoch 74/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6152 - accuracy: 0.6715\n",
      "Epoch 75/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6161 - accuracy: 0.6802\n",
      "Epoch 76/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6109 - accuracy: 0.6773\n",
      "Epoch 77/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6113 - accuracy: 0.6729\n",
      "Epoch 78/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6146 - accuracy: 0.6758\n",
      "Epoch 79/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6094 - accuracy: 0.6729\n",
      "Epoch 80/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6124 - accuracy: 0.6744\n",
      "Epoch 81/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6190 - accuracy: 0.6744\n",
      "Epoch 82/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.6758\n",
      "Epoch 83/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6079 - accuracy: 0.6787\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6065 - accuracy: 0.6816\n",
      "Epoch 85/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6099 - accuracy: 0.6816\n",
      "Epoch 86/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6061 - accuracy: 0.6744\n",
      "Epoch 87/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6059 - accuracy: 0.6802\n",
      "Epoch 88/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6098 - accuracy: 0.6773\n",
      "Epoch 89/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.6758\n",
      "Epoch 90/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6078 - accuracy: 0.6802\n",
      "Epoch 91/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6130 - accuracy: 0.6715\n",
      "Epoch 92/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6028 - accuracy: 0.6816\n",
      "Epoch 93/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6047 - accuracy: 0.6787\n",
      "Epoch 94/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.6787\n",
      "Epoch 95/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6004 - accuracy: 0.6889\n",
      "Epoch 96/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6050 - accuracy: 0.6758\n",
      "Epoch 97/150\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6097 - accuracy: 0.6787\n",
      "Epoch 98/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6030 - accuracy: 0.6700\n",
      "Epoch 99/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.6802\n",
      "Epoch 100/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6011 - accuracy: 0.6773\n",
      "Epoch 101/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6061 - accuracy: 0.6816\n",
      "Epoch 102/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.6845\n",
      "Epoch 103/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6033 - accuracy: 0.6802\n",
      "Epoch 104/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5987 - accuracy: 0.6787\n",
      "Epoch 105/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5984 - accuracy: 0.6831\n",
      "Epoch 106/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5937 - accuracy: 0.6874\n",
      "Epoch 107/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5984 - accuracy: 0.6831\n",
      "Epoch 108/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5988 - accuracy: 0.6787\n",
      "Epoch 109/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5998 - accuracy: 0.6744\n",
      "Epoch 110/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5959 - accuracy: 0.6816\n",
      "Epoch 111/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5944 - accuracy: 0.6802\n",
      "Epoch 112/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5988 - accuracy: 0.6845\n",
      "Epoch 113/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5985 - accuracy: 0.6787\n",
      "Epoch 114/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5939 - accuracy: 0.6874\n",
      "Epoch 115/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5913 - accuracy: 0.6816\n",
      "Epoch 116/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5914 - accuracy: 0.6802\n",
      "Epoch 117/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5958 - accuracy: 0.6831\n",
      "Epoch 118/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5914 - accuracy: 0.6845\n",
      "Epoch 119/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5904 - accuracy: 0.6816\n",
      "Epoch 120/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5919 - accuracy: 0.6874\n",
      "Epoch 121/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5951 - accuracy: 0.6787\n",
      "Epoch 122/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5918 - accuracy: 0.6816\n",
      "Epoch 123/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5904 - accuracy: 0.6860\n",
      "Epoch 124/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5898 - accuracy: 0.6961\n",
      "Epoch 125/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5884 - accuracy: 0.6831\n",
      "Epoch 126/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5909 - accuracy: 0.6831\n",
      "Epoch 127/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5876 - accuracy: 0.6860\n",
      "Epoch 128/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5895 - accuracy: 0.6802\n",
      "Epoch 129/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5905 - accuracy: 0.6831\n",
      "Epoch 130/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.6845\n",
      "Epoch 131/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5896 - accuracy: 0.6903\n",
      "Epoch 132/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.6918\n",
      "Epoch 133/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5886 - accuracy: 0.6802\n",
      "Epoch 134/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5872 - accuracy: 0.6874\n",
      "Epoch 135/150\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5875 - accuracy: 0.6831\n",
      "Epoch 136/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5965 - accuracy: 0.6860\n",
      "Epoch 137/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5846 - accuracy: 0.6860\n",
      "Epoch 138/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5845 - accuracy: 0.6802\n",
      "Epoch 139/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5853 - accuracy: 0.6860\n",
      "Epoch 140/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5831 - accuracy: 0.6889\n",
      "Epoch 141/150\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5867 - accuracy: 0.6773\n",
      "Epoch 142/150\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5852 - accuracy: 0.6874\n",
      "Epoch 143/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5862 - accuracy: 0.6816\n",
      "Epoch 144/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5856 - accuracy: 0.6932\n",
      "Epoch 145/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.6874\n",
      "Epoch 146/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5851 - accuracy: 0.6845\n",
      "Epoch 147/150\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.6845\n",
      "Epoch 148/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5867 - accuracy: 0.6860\n",
      "Epoch 149/150\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5855 - accuracy: 0.6802\n",
      "Epoch 150/150\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5852 - accuracy: 0.6918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c7bbee50d0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,\n",
    "          y_train, \n",
    "          epochs=150, \n",
    "          batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "70/70 [==============================] - 1s 2ms/step - loss: 7.5963 - accuracy: 0.3444\n",
      "Epoch 2/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 1.7929 - accuracy: 0.3444\n",
      "Epoch 3/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 1.1883 - accuracy: 0.3444\n",
      "Epoch 4/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.9848 - accuracy: 0.3444\n",
      "Epoch 5/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.8276 - accuracy: 0.3444\n",
      "Epoch 6/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.7533 - accuracy: 0.3444\n",
      "Epoch 7/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.7165 - accuracy: 0.3444\n",
      "Epoch 8/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.3444\n",
      "Epoch 9/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6704 - accuracy: 0.3444\n",
      "Epoch 10/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.3444\n",
      "Epoch 11/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.3444\n",
      "Epoch 12/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6705 - accuracy: 0.3444\n",
      "Epoch 13/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6296 - accuracy: 0.3444\n",
      "Epoch 14/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6409 - accuracy: 0.3444\n",
      "Epoch 15/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.3444\n",
      "Epoch 16/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7618 - accuracy: 0.3444\n",
      "Epoch 17/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6160 - accuracy: 0.3444\n",
      "Epoch 18/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.3444\n",
      "Epoch 19/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6301 - accuracy: 0.3444\n",
      "Epoch 20/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6246 - accuracy: 0.3444\n",
      "Epoch 21/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6145 - accuracy: 0.3444\n",
      "Epoch 22/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5853 - accuracy: 0.3444\n",
      "Epoch 23/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6107 - accuracy: 0.3444\n",
      "Epoch 24/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5911 - accuracy: 0.3444\n",
      "Epoch 25/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6079 - accuracy: 0.3444\n",
      "Epoch 26/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5898 - accuracy: 0.3444\n",
      "Epoch 27/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5871 - accuracy: 0.3444\n",
      "Epoch 28/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6038 - accuracy: 0.3444\n",
      "Epoch 29/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.6371 - accuracy: 0.3444\n",
      "Epoch 30/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5925 - accuracy: 0.3444\n",
      "Epoch 31/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6416 - accuracy: 0.3444\n",
      "Epoch 32/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.3444\n",
      "Epoch 33/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.3444\n",
      "Epoch 34/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6164 - accuracy: 0.3444\n",
      "Epoch 35/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5739 - accuracy: 0.3444\n",
      "Epoch 36/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5733 - accuracy: 0.3444\n",
      "Epoch 37/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5917 - accuracy: 0.3444\n",
      "Epoch 38/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.3444\n",
      "Epoch 39/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5844 - accuracy: 0.3444\n",
      "Epoch 40/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.3444\n",
      "Epoch 41/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5674 - accuracy: 0.3444\n",
      "Epoch 42/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5702 - accuracy: 0.3444\n",
      "Epoch 43/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5699 - accuracy: 0.3444\n",
      "Epoch 44/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6005 - accuracy: 0.3444\n",
      "Epoch 45/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5652 - accuracy: 0.3444\n",
      "Epoch 46/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.3444\n",
      "Epoch 47/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5564 - accuracy: 0.3444\n",
      "Epoch 48/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5666 - accuracy: 0.3444\n",
      "Epoch 49/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5777 - accuracy: 0.3444\n",
      "Epoch 50/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5646 - accuracy: 0.3444\n",
      "Epoch 51/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5685 - accuracy: 0.3444\n",
      "Epoch 52/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.3444\n",
      "Epoch 53/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5492 - accuracy: 0.3444\n",
      "Epoch 54/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5588 - accuracy: 0.3444\n",
      "Epoch 55/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.3444\n",
      "Epoch 56/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5444 - accuracy: 0.3444\n",
      "Epoch 57/150\n",
      "70/70 [==============================] - -5s -71082us/step - loss: 0.5552 - accuracy: 0.3444\n",
      "Epoch 58/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.3444\n",
      "Epoch 59/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.3444\n",
      "Epoch 60/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.3444\n",
      "Epoch 61/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.3444\n",
      "Epoch 62/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.3444\n",
      "Epoch 63/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.3444\n",
      "Epoch 64/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.3444\n",
      "Epoch 65/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.3444\n",
      "Epoch 66/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.3444\n",
      "Epoch 67/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.3444\n",
      "Epoch 68/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5727 - accuracy: 0.3444\n",
      "Epoch 69/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5395 - accuracy: 0.3444\n",
      "Epoch 70/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5455 - accuracy: 0.3444\n",
      "Epoch 71/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.3444\n",
      "Epoch 72/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5549 - accuracy: 0.3444\n",
      "Epoch 73/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5300 - accuracy: 0.3444\n",
      "Epoch 74/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.3444\n",
      "Epoch 75/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.3444\n",
      "Epoch 76/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.3444\n",
      "Epoch 77/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5956 - accuracy: 0.3444\n",
      "Epoch 78/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.3444\n",
      "Epoch 79/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.3444\n",
      "Epoch 80/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.3444\n",
      "Epoch 81/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5554 - accuracy: 0.3444\n",
      "Epoch 82/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.3444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.3444\n",
      "Epoch 84/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.3444\n",
      "Epoch 85/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5484 - accuracy: 0.3444\n",
      "Epoch 86/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5210 - accuracy: 0.3444\n",
      "Epoch 87/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.3444\n",
      "Epoch 88/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.3444\n",
      "Epoch 89/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.3444\n",
      "Epoch 90/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.3444\n",
      "Epoch 91/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.3444\n",
      "Epoch 92/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.3444\n",
      "Epoch 93/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.3444\n",
      "Epoch 94/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.3444\n",
      "Epoch 95/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.3444\n",
      "Epoch 96/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.3444\n",
      "Epoch 97/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.3444\n",
      "Epoch 98/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5419 - accuracy: 0.3444\n",
      "Epoch 99/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.3444\n",
      "Epoch 100/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.3444\n",
      "Epoch 101/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.3444\n",
      "Epoch 102/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.3444\n",
      "Epoch 103/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5467 - accuracy: 0.3444\n",
      "Epoch 104/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.3444\n",
      "Epoch 105/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.3444\n",
      "Epoch 106/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.3444\n",
      "Epoch 107/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.3444\n",
      "Epoch 108/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.3444\n",
      "Epoch 109/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.3444\n",
      "Epoch 110/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.3444\n",
      "Epoch 111/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.3444\n",
      "Epoch 112/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.3444\n",
      "Epoch 113/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.3444\n",
      "Epoch 114/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.3444\n",
      "Epoch 115/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6047 - accuracy: 0.3444\n",
      "Epoch 116/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.3444\n",
      "Epoch 117/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6101 - accuracy: 0.3444\n",
      "Epoch 118/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.3444\n",
      "Epoch 119/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.3444\n",
      "Epoch 120/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.3444\n",
      "Epoch 121/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.3444\n",
      "Epoch 122/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.3444\n",
      "Epoch 123/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.3444\n",
      "Epoch 124/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4983 - accuracy: 0.3444\n",
      "Epoch 125/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5148 - accuracy: 0.3444\n",
      "Epoch 126/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5082 - accuracy: 0.3444\n",
      "Epoch 127/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4991 - accuracy: 0.3444\n",
      "Epoch 128/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4942 - accuracy: 0.3444\n",
      "Epoch 129/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.3444\n",
      "Epoch 130/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4996 - accuracy: 0.3444\n",
      "Epoch 131/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4875 - accuracy: 0.3444\n",
      "Epoch 132/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5303 - accuracy: 0.3444\n",
      "Epoch 133/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5026 - accuracy: 0.3444\n",
      "Epoch 134/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5039 - accuracy: 0.3444\n",
      "Epoch 135/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4994 - accuracy: 0.3444\n",
      "Epoch 136/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.3444\n",
      "Epoch 137/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.3444\n",
      "Epoch 138/150\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.5009 - accuracy: 0.3444\n",
      "Epoch 139/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.3444\n",
      "Epoch 140/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.3444\n",
      "Epoch 141/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.3444\n",
      "Epoch 142/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.3444\n",
      "Epoch 143/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.3444\n",
      "Epoch 144/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4912 - accuracy: 0.3444\n",
      "Epoch 145/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5147 - accuracy: 0.3444\n",
      "Epoch 146/150\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4943 - accuracy: 0.3444\n",
      "Epoch 147/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.3444\n",
      "Epoch 148/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5551 - accuracy: 0.3444\n",
      "Epoch 149/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.3444\n",
      "Epoch 150/150\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.3444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c7bfe77cd0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, \n",
    "           y_train, \n",
    "           epochs=150, \n",
    "           batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 12)                108       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 12)                108       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(8, 12) dtype=float32, numpy=\n",
       " array([[-0.2633508 , -0.45748353,  0.28528008,  0.33984384, -0.5265822 ,\n",
       "          0.30255365,  0.59656596,  0.13808113,  0.19055045,  0.07583866,\n",
       "         -0.25269574, -0.17442778],\n",
       "        [-0.00214182, -0.19275872,  0.10444093,  0.23727904,  0.39408547,\n",
       "          0.24455269,  0.52065796, -0.43859476,  0.22361405, -0.26925373,\n",
       "          0.30348548,  0.32821232],\n",
       "        [ 0.13441545, -0.1417122 , -0.3231773 , -0.30571622,  0.26332763,\n",
       "         -0.31829688,  0.34918433, -0.50763035, -0.57098335,  0.45681748,\n",
       "         -0.5491876 ,  0.16720685],\n",
       "        [ 0.19895011,  0.32772654,  0.2517244 ,  0.47846925, -0.10523549,\n",
       "         -0.00744017,  0.07183918, -0.4017138 ,  0.10229038, -0.40606302,\n",
       "         -0.36382636,  0.67181486],\n",
       "        [ 0.2773437 , -0.5386601 ,  0.29966092,  0.07656346, -0.34599245,\n",
       "         -0.27524707, -0.4854995 , -0.16942897, -0.54066813,  0.43971568,\n",
       "          0.30005878, -0.42161077],\n",
       "        [-0.43399987,  0.18219039,  0.2893623 ,  0.22911981, -0.02967446,\n",
       "          0.11169349,  0.05556249,  0.14856648,  0.07813536,  0.04836551,\n",
       "          0.15416716, -0.21268934],\n",
       "        [-0.17644538,  0.285203  ,  0.26556018, -0.21848698, -0.65811175,\n",
       "          0.26716003, -0.23171672, -0.22356966, -0.00981889, -0.1412722 ,\n",
       "         -0.11347421,  0.24799751],\n",
       "        [-0.05431248,  0.30706266, -0.34707117,  0.03260273,  0.44726616,\n",
       "          0.01465004,  0.23870064, -0.36737365, -0.44370586, -0.67013884,\n",
       "         -0.03402696,  0.43817723]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(12,) dtype=float32, numpy=\n",
       " array([-0.19779882,  0.00218716, -0.10304596,  0.23745039,  0.26270464,\n",
       "        -0.28396872, -0.28811166,  0.        , -0.05814631,  0.1538064 ,\n",
       "         0.08233125,  0.20142914], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(12, 8) dtype=float32, numpy=\n",
       " array([[ 1.25158176e-01,  3.57519001e-01, -5.73369443e-01,\n",
       "          5.67376614e-04,  3.77449632e-01,  4.09521371e-01,\n",
       "          9.19427052e-02, -4.05872464e-01],\n",
       "        [-4.47211266e-01,  3.50219309e-01, -1.63138166e-01,\n",
       "          2.10114360e-01, -4.17392589e-02, -1.55725822e-01,\n",
       "          3.70865494e-01, -5.30093849e-01],\n",
       "        [ 4.62973118e-01,  1.67159021e-01, -4.58523273e-01,\n",
       "          2.45563686e-01,  3.45814228e-01,  1.57015413e-01,\n",
       "         -1.22284755e-01, -5.42613626e-01],\n",
       "        [-1.23086214e-01, -1.13204941e-01, -2.18633071e-01,\n",
       "         -1.00361258e-01, -2.47118533e-01, -1.65568262e-01,\n",
       "          1.28677502e-01, -4.11659688e-01],\n",
       "        [ 1.93192527e-01,  1.92034304e-01, -4.19212103e-01,\n",
       "         -2.55344808e-01, -1.53274462e-01,  5.55994827e-03,\n",
       "          2.14061126e-01, -1.79592967e-01],\n",
       "        [-4.68062818e-01, -3.05741634e-02, -1.64003342e-01,\n",
       "         -3.24387431e-01,  3.51926625e-01, -5.41540980e-01,\n",
       "         -3.12111199e-01, -4.42234993e-01],\n",
       "        [ 3.61381620e-02,  4.75128502e-01, -5.48590124e-02,\n",
       "         -3.51029634e-03,  3.50266784e-01,  1.76048830e-01,\n",
       "          4.00892437e-01, -2.26020664e-01],\n",
       "        [-2.34406710e-01,  1.96932971e-01, -1.95031255e-01,\n",
       "          5.28272271e-01, -2.96028733e-01, -2.92710066e-02,\n",
       "         -2.92673677e-01,  3.58491898e-01],\n",
       "        [-6.17441714e-01, -2.67782807e-01, -5.34868121e-01,\n",
       "         -1.52825713e-01,  2.33809918e-01, -3.61793846e-01,\n",
       "          1.47537529e-01,  5.57714701e-03],\n",
       "        [-1.01874061e-01, -4.07104999e-01, -1.65877551e-01,\n",
       "         -4.65086848e-01,  5.12053892e-02,  3.38442475e-01,\n",
       "          4.52542305e-01, -3.28008235e-01],\n",
       "        [-3.66274416e-01,  5.95219210e-02,  2.61955678e-01,\n",
       "         -4.25465643e-01,  1.09595224e-01, -2.01192036e-01,\n",
       "          4.40581918e-01, -1.59369051e-01],\n",
       "        [-9.17790271e-03, -8.14589709e-02,  3.98259014e-01,\n",
       "         -3.23879063e-01, -3.87422383e-01, -2.47234255e-01,\n",
       "         -1.92415088e-01,  3.23330879e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(8,) dtype=float32, numpy=\n",
       " array([ 0.48030037, -0.37165797, -0.04967251,  0.        , -0.19852227,\n",
       "        -0.18350978,  0.32836252,  0.        ], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(8, 1) dtype=float32, numpy=\n",
       " array([[-0.20599018],\n",
       "        [ 0.36838922],\n",
       "        [-0.7440933 ],\n",
       "        [ 0.4129914 ],\n",
       "        [ 0.47184926],\n",
       "        [ 0.1143186 ],\n",
       "        [-0.4110583 ],\n",
       "        [ 0.4438188 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(1,) dtype=float32, numpy=array([-0.33839557], dtype=float32)>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(8, 12) dtype=float32, numpy=\n",
       " array([[-0.2633508 , -0.45748353,  0.28528008,  0.33984384, -0.5265822 ,\n",
       "          0.30255365,  0.59656596,  0.13808113,  0.19055045,  0.07583866,\n",
       "         -0.25269574, -0.17442778],\n",
       "        [-0.00214182, -0.19275872,  0.10444093,  0.23727904,  0.39408547,\n",
       "          0.24455269,  0.52065796, -0.43859476,  0.22361405, -0.26925373,\n",
       "          0.30348548,  0.32821232],\n",
       "        [ 0.13441545, -0.1417122 , -0.3231773 , -0.30571622,  0.26332763,\n",
       "         -0.31829688,  0.34918433, -0.50763035, -0.57098335,  0.45681748,\n",
       "         -0.5491876 ,  0.16720685],\n",
       "        [ 0.19895011,  0.32772654,  0.2517244 ,  0.47846925, -0.10523549,\n",
       "         -0.00744017,  0.07183918, -0.4017138 ,  0.10229038, -0.40606302,\n",
       "         -0.36382636,  0.67181486],\n",
       "        [ 0.2773437 , -0.5386601 ,  0.29966092,  0.07656346, -0.34599245,\n",
       "         -0.27524707, -0.4854995 , -0.16942897, -0.54066813,  0.43971568,\n",
       "          0.30005878, -0.42161077],\n",
       "        [-0.43399987,  0.18219039,  0.2893623 ,  0.22911981, -0.02967446,\n",
       "          0.11169349,  0.05556249,  0.14856648,  0.07813536,  0.04836551,\n",
       "          0.15416716, -0.21268934],\n",
       "        [-0.17644538,  0.285203  ,  0.26556018, -0.21848698, -0.65811175,\n",
       "          0.26716003, -0.23171672, -0.22356966, -0.00981889, -0.1412722 ,\n",
       "         -0.11347421,  0.24799751],\n",
       "        [-0.05431248,  0.30706266, -0.34707117,  0.03260273,  0.44726616,\n",
       "          0.01465004,  0.23870064, -0.36737365, -0.44370586, -0.67013884,\n",
       "         -0.03402696,  0.43817723]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(12,) dtype=float32, numpy=\n",
       " array([-0.19779882,  0.00218716, -0.10304596,  0.23745039,  0.26270464,\n",
       "        -0.28396872, -0.28811166,  0.        , -0.05814631,  0.1538064 ,\n",
       "         0.08233125,  0.20142914], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(12, 8) dtype=float32, numpy=\n",
       " array([[ 1.25158176e-01,  3.57519001e-01, -5.73369443e-01,\n",
       "          5.67376614e-04,  3.77449632e-01,  4.09521371e-01,\n",
       "          9.19427052e-02, -4.05872464e-01],\n",
       "        [-4.47211266e-01,  3.50219309e-01, -1.63138166e-01,\n",
       "          2.10114360e-01, -4.17392589e-02, -1.55725822e-01,\n",
       "          3.70865494e-01, -5.30093849e-01],\n",
       "        [ 4.62973118e-01,  1.67159021e-01, -4.58523273e-01,\n",
       "          2.45563686e-01,  3.45814228e-01,  1.57015413e-01,\n",
       "         -1.22284755e-01, -5.42613626e-01],\n",
       "        [-1.23086214e-01, -1.13204941e-01, -2.18633071e-01,\n",
       "         -1.00361258e-01, -2.47118533e-01, -1.65568262e-01,\n",
       "          1.28677502e-01, -4.11659688e-01],\n",
       "        [ 1.93192527e-01,  1.92034304e-01, -4.19212103e-01,\n",
       "         -2.55344808e-01, -1.53274462e-01,  5.55994827e-03,\n",
       "          2.14061126e-01, -1.79592967e-01],\n",
       "        [-4.68062818e-01, -3.05741634e-02, -1.64003342e-01,\n",
       "         -3.24387431e-01,  3.51926625e-01, -5.41540980e-01,\n",
       "         -3.12111199e-01, -4.42234993e-01],\n",
       "        [ 3.61381620e-02,  4.75128502e-01, -5.48590124e-02,\n",
       "         -3.51029634e-03,  3.50266784e-01,  1.76048830e-01,\n",
       "          4.00892437e-01, -2.26020664e-01],\n",
       "        [-2.34406710e-01,  1.96932971e-01, -1.95031255e-01,\n",
       "          5.28272271e-01, -2.96028733e-01, -2.92710066e-02,\n",
       "         -2.92673677e-01,  3.58491898e-01],\n",
       "        [-6.17441714e-01, -2.67782807e-01, -5.34868121e-01,\n",
       "         -1.52825713e-01,  2.33809918e-01, -3.61793846e-01,\n",
       "          1.47537529e-01,  5.57714701e-03],\n",
       "        [-1.01874061e-01, -4.07104999e-01, -1.65877551e-01,\n",
       "         -4.65086848e-01,  5.12053892e-02,  3.38442475e-01,\n",
       "          4.52542305e-01, -3.28008235e-01],\n",
       "        [-3.66274416e-01,  5.95219210e-02,  2.61955678e-01,\n",
       "         -4.25465643e-01,  1.09595224e-01, -2.01192036e-01,\n",
       "          4.40581918e-01, -1.59369051e-01],\n",
       "        [-9.17790271e-03, -8.14589709e-02,  3.98259014e-01,\n",
       "         -3.23879063e-01, -3.87422383e-01, -2.47234255e-01,\n",
       "         -1.92415088e-01,  3.23330879e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(8,) dtype=float32, numpy=\n",
       " array([ 0.48030037, -0.37165797, -0.04967251,  0.        , -0.19852227,\n",
       "        -0.18350978,  0.32836252,  0.        ], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(8, 1) dtype=float32, numpy=\n",
       " array([[-0.20599018],\n",
       "        [ 0.36838922],\n",
       "        [-0.7440933 ],\n",
       "        [ 0.4129914 ],\n",
       "        [ 0.47184926],\n",
       "        [ 0.1143186 ],\n",
       "        [-0.4110583 ],\n",
       "        [ 0.4438188 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(1,) dtype=float32, numpy=array([-0.33839557], dtype=float32)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.metrics.base_metric.Mean at 0x2c7bbee51f0>,\n",
       " <keras.metrics.base_metric.MeanMetricWrapper at 0x2c7be231f40>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5795 - accuracy: 0.6889\n",
      "[0.5794796347618103, 0.6888567209243774]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate the keras model\n",
    "\n",
    "params = model.evaluate(X_train, y_train)\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.3444\n",
      "[0.5304805040359497, 0.3444283604621887]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate the keras model\n",
    "\n",
    "params2 = model2.evaluate(X_train, y_train)\n",
    "print(params2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first is  the loss of the model on the dataset \n",
    "#the second will be the accuracy of the model on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.14667349],\n",
       "       [0.18157528],\n",
       "       [0.28559655],\n",
       "       [0.14897549],\n",
       "       [0.38124534],\n",
       "       [0.6498383 ],\n",
       "       [0.4109817 ],\n",
       "       [0.12690237],\n",
       "       [0.91076505],\n",
       "       [0.19322655],\n",
       "       [0.0069039 ],\n",
       "       [0.8091983 ],\n",
       "       [0.58659095],\n",
       "       [0.25062224],\n",
       "       [0.35774732],\n",
       "       [0.14980187],\n",
       "       [0.25499392],\n",
       "       [0.14427473],\n",
       "       [0.17637141],\n",
       "       [0.32186794],\n",
       "       [0.13144024],\n",
       "       [0.20943865],\n",
       "       [0.71331435],\n",
       "       [0.6668864 ],\n",
       "       [0.27293494],\n",
       "       [0.63107455],\n",
       "       [0.36924008],\n",
       "       [0.16735584],\n",
       "       [0.20530483],\n",
       "       [0.43137375],\n",
       "       [0.02294877],\n",
       "       [0.90143865],\n",
       "       [0.08931841],\n",
       "       [0.04534633],\n",
       "       [0.18640952],\n",
       "       [0.31388754],\n",
       "       [0.8458572 ],\n",
       "       [0.32570264],\n",
       "       [0.05751811],\n",
       "       [0.6808381 ],\n",
       "       [0.39678296],\n",
       "       [0.705061  ],\n",
       "       [0.46107948],\n",
       "       [0.3766969 ],\n",
       "       [0.17822371],\n",
       "       [0.4763961 ],\n",
       "       [0.20119369],\n",
       "       [0.1496479 ],\n",
       "       [0.8808442 ],\n",
       "       [0.9323847 ],\n",
       "       [0.01571124],\n",
       "       [0.18721488],\n",
       "       [0.58897424],\n",
       "       [0.77609545],\n",
       "       [0.50176823],\n",
       "       [0.54439753],\n",
       "       [0.1164915 ],\n",
       "       [0.16549294],\n",
       "       [0.7326797 ],\n",
       "       [0.3093432 ],\n",
       "       [0.11466391],\n",
       "       [0.6826916 ],\n",
       "       [0.38204342],\n",
       "       [0.57327706],\n",
       "       [0.3687876 ],\n",
       "       [0.2506581 ],\n",
       "       [0.43060708],\n",
       "       [0.04393439],\n",
       "       [0.2164962 ],\n",
       "       [0.03450957],\n",
       "       [0.23437867],\n",
       "       [0.40704623],\n",
       "       [0.39082056],\n",
       "       [0.41031098],\n",
       "       [0.26148772],\n",
       "       [0.32866296],\n",
       "       [0.5823511 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "params = model.evaluate(X_test, y_test)\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ".152719170     ----0\n",
    ".116------------0\n",
    ".54-----------------1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# round predictions \n",
    "\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "rounded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  7]\n",
      " [16 14]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, rounded))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, rounded), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7012987012987013"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    56\n",
       "1    21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rounded).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
