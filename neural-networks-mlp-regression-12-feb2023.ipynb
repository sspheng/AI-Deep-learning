{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDY\n",
    "\n",
    "## https://www.asimovinstitute.org/neural-network-zoo/\n",
    "\n",
    "\n",
    "\n",
    "# PLAYGROUND \n",
    "\n",
    "\n",
    "## https://playground.tensorflow.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING SKLEARN NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes=(2,3,5),\n",
    "                   verbose=2,\n",
    "                   activation=\"logistic\" ,\n",
    "                   batch_size=40,\n",
    "                   random_state=1, \n",
    "                   max_iter=2000,\n",
    "                   learning_rate =\"adaptive\",\n",
    "                   solver='sgd',\n",
    "                   learning_rate_init=0.0001)\n",
    "\n",
    "\n",
    "# 2 neurons and then 3 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "??MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "??MLPRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ON RANDOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[22,33],\n",
    "           [44,55]]\n",
    "\n",
    "y_train = [22,34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 390.83401686\n",
      "Iteration 2, loss = 390.48494891\n",
      "Iteration 3, loss = 389.98746843\n",
      "Iteration 4, loss = 389.35682131\n",
      "Iteration 5, loss = 388.60680103\n",
      "Iteration 6, loss = 387.74987570\n",
      "Iteration 7, loss = 386.79730551\n",
      "Iteration 8, loss = 385.75925097\n",
      "Iteration 9, loss = 384.64487260\n",
      "Iteration 10, loss = 383.46242244\n",
      "Iteration 11, loss = 382.21932806\n",
      "Iteration 12, loss = 380.92226937\n",
      "Iteration 13, loss = 379.57724895\n",
      "Iteration 14, loss = 378.18965620\n",
      "Iteration 15, loss = 376.76432583\n",
      "Iteration 16, loss = 375.30559122\n",
      "Iteration 17, loss = 373.81733290\n",
      "Iteration 18, loss = 372.30302264\n",
      "Iteration 19, loss = 370.76576350\n",
      "Iteration 20, loss = 369.20832619\n",
      "Iteration 21, loss = 367.63318203\n",
      "Iteration 22, loss = 366.04253279\n",
      "Iteration 23, loss = 364.43833774\n",
      "Iteration 24, loss = 362.82233815\n",
      "Iteration 25, loss = 361.19607939\n",
      "Iteration 26, loss = 359.56093095\n",
      "Iteration 27, loss = 357.91810451\n",
      "Iteration 28, loss = 356.26867030\n",
      "Iteration 29, loss = 354.61357177\n",
      "Iteration 30, loss = 352.95363894\n",
      "Iteration 31, loss = 351.28960035\n",
      "Iteration 32, loss = 349.62209393\n",
      "Iteration 33, loss = 347.95167667\n",
      "Iteration 34, loss = 346.27883344\n",
      "Iteration 35, loss = 344.60398490\n",
      "Iteration 36, loss = 342.92749460\n",
      "Iteration 37, loss = 341.24967541\n",
      "Iteration 38, loss = 339.57079527\n",
      "Iteration 39, loss = 337.89108238\n",
      "Iteration 40, loss = 336.21072990\n",
      "Iteration 41, loss = 334.52990010\n",
      "Iteration 42, loss = 332.84872817\n",
      "Iteration 43, loss = 331.16732559\n",
      "Iteration 44, loss = 329.48578321\n",
      "Iteration 45, loss = 327.80417396\n",
      "Iteration 46, loss = 326.12255530\n",
      "Iteration 47, loss = 324.44097148\n",
      "Iteration 48, loss = 322.75945546\n",
      "Iteration 49, loss = 321.07803074\n",
      "Iteration 50, loss = 319.39671291\n",
      "Iteration 51, loss = 317.71551112\n",
      "Iteration 52, loss = 316.03442933\n",
      "Iteration 53, loss = 314.35346747\n",
      "Iteration 54, loss = 312.67262246\n",
      "Iteration 55, loss = 310.99188912\n",
      "Iteration 56, loss = 309.31126101\n",
      "Iteration 57, loss = 307.63073112\n",
      "Iteration 58, loss = 305.95029254\n",
      "Iteration 59, loss = 304.26993905\n",
      "Iteration 60, loss = 302.58966560\n",
      "Iteration 61, loss = 300.90946877\n",
      "Iteration 62, loss = 299.22934716\n",
      "Iteration 63, loss = 297.54930176\n",
      "Iteration 64, loss = 295.86933623\n",
      "Iteration 65, loss = 294.18945718\n",
      "Iteration 66, loss = 292.50967438\n",
      "Iteration 67, loss = 290.83000095\n",
      "Iteration 68, loss = 289.15045357\n",
      "Iteration 69, loss = 287.47105254\n",
      "Iteration 70, loss = 285.79182195\n",
      "Iteration 71, loss = 284.11278976\n",
      "Iteration 72, loss = 282.43398783\n",
      "Iteration 73, loss = 280.75545201\n",
      "Iteration 74, loss = 279.07722216\n",
      "Iteration 75, loss = 277.39934213\n",
      "Iteration 76, loss = 275.72185979\n",
      "Iteration 77, loss = 274.04482704\n",
      "Iteration 78, loss = 272.36829970\n",
      "Iteration 79, loss = 270.69233757\n",
      "Iteration 80, loss = 269.01700430\n",
      "Iteration 81, loss = 267.34236738\n",
      "Iteration 82, loss = 265.66849802\n",
      "Iteration 83, loss = 263.99547115\n",
      "Iteration 84, loss = 262.32336525\n",
      "Iteration 85, loss = 260.65226232\n",
      "Iteration 86, loss = 258.98224775\n",
      "Iteration 87, loss = 257.31341021\n",
      "Iteration 88, loss = 255.64584158\n",
      "Iteration 89, loss = 253.97963678\n",
      "Iteration 90, loss = 252.31489370\n",
      "Iteration 91, loss = 250.65171304\n",
      "Iteration 92, loss = 248.99019821\n",
      "Iteration 93, loss = 247.33045517\n",
      "Iteration 94, loss = 245.67259234\n",
      "Iteration 95, loss = 244.01672040\n",
      "Iteration 96, loss = 242.36295224\n",
      "Iteration 97, loss = 240.71140274\n",
      "Iteration 98, loss = 239.06218867\n",
      "Iteration 99, loss = 237.41542854\n",
      "Iteration 100, loss = 235.77124247\n",
      "Iteration 101, loss = 234.12975202\n",
      "Iteration 102, loss = 232.49108009\n",
      "Iteration 103, loss = 230.85535072\n",
      "Iteration 104, loss = 229.22268900\n",
      "Iteration 105, loss = 227.59322093\n",
      "Iteration 106, loss = 225.96707321\n",
      "Iteration 107, loss = 224.34437321\n",
      "Iteration 108, loss = 222.72524873\n",
      "Iteration 109, loss = 221.10982794\n",
      "Iteration 110, loss = 219.49823919\n",
      "Iteration 111, loss = 217.89061094\n",
      "Iteration 112, loss = 216.28707157\n",
      "Iteration 113, loss = 214.68774929\n",
      "Iteration 114, loss = 213.09277200\n",
      "Iteration 115, loss = 211.50226719\n",
      "Iteration 116, loss = 209.91636180\n",
      "Iteration 117, loss = 208.33518211\n",
      "Iteration 118, loss = 206.75885364\n",
      "Iteration 119, loss = 205.18750103\n",
      "Iteration 120, loss = 203.62124793\n",
      "Iteration 121, loss = 202.06021694\n",
      "Iteration 122, loss = 200.50452943\n",
      "Iteration 123, loss = 198.95430554\n",
      "Iteration 124, loss = 197.40966401\n",
      "Iteration 125, loss = 195.87072217\n",
      "Iteration 126, loss = 194.33759576\n",
      "Iteration 127, loss = 192.81039896\n",
      "Iteration 128, loss = 191.28924423\n",
      "Iteration 129, loss = 189.77424227\n",
      "Iteration 130, loss = 188.26550197\n",
      "Iteration 131, loss = 186.76313030\n",
      "Iteration 132, loss = 185.26723231\n",
      "Iteration 133, loss = 183.77791100\n",
      "Iteration 134, loss = 182.29526734\n",
      "Iteration 135, loss = 180.81940017\n",
      "Iteration 136, loss = 179.35040619\n",
      "Iteration 137, loss = 177.88837987\n",
      "Iteration 138, loss = 176.43341348\n",
      "Iteration 139, loss = 174.98559698\n",
      "Iteration 140, loss = 173.54501805\n",
      "Iteration 141, loss = 172.11176202\n",
      "Iteration 142, loss = 170.68591187\n",
      "Iteration 143, loss = 169.26754820\n",
      "Iteration 144, loss = 167.85674919\n",
      "Iteration 145, loss = 166.45359063\n",
      "Iteration 146, loss = 165.05814586\n",
      "Iteration 147, loss = 163.67048578\n",
      "Iteration 148, loss = 162.29067886\n",
      "Iteration 149, loss = 160.91879109\n",
      "Iteration 150, loss = 159.55488601\n",
      "Iteration 151, loss = 158.19902473\n",
      "Iteration 152, loss = 156.85126585\n",
      "Iteration 153, loss = 155.51166558\n",
      "Iteration 154, loss = 154.18027765\n",
      "Iteration 155, loss = 152.85715336\n",
      "Iteration 156, loss = 151.54234159\n",
      "Iteration 157, loss = 150.23588879\n",
      "Iteration 158, loss = 148.93783905\n",
      "Iteration 159, loss = 147.64823404\n",
      "Iteration 160, loss = 146.36711308\n",
      "Iteration 161, loss = 145.09451314\n",
      "Iteration 162, loss = 143.83046889\n",
      "Iteration 163, loss = 142.57501265\n",
      "Iteration 164, loss = 141.32817450\n",
      "Iteration 165, loss = 140.08998224\n",
      "Iteration 166, loss = 138.86046148\n",
      "Iteration 167, loss = 137.63963557\n",
      "Iteration 168, loss = 136.42752574\n",
      "Iteration 169, loss = 135.22415103\n",
      "Iteration 170, loss = 134.02952840\n",
      "Iteration 171, loss = 132.84367271\n",
      "Iteration 172, loss = 131.66659676\n",
      "Iteration 173, loss = 130.49831133\n",
      "Iteration 174, loss = 129.33882522\n",
      "Iteration 175, loss = 128.18814526\n",
      "Iteration 176, loss = 127.04627638\n",
      "Iteration 177, loss = 125.91322159\n",
      "Iteration 178, loss = 124.78898206\n",
      "Iteration 179, loss = 123.67355715\n",
      "Iteration 180, loss = 122.56694443\n",
      "Iteration 181, loss = 121.46913971\n",
      "Iteration 182, loss = 120.38013709\n",
      "Iteration 183, loss = 119.29992901\n",
      "Iteration 184, loss = 118.22850625\n",
      "Iteration 185, loss = 117.16585799\n",
      "Iteration 186, loss = 116.11197183\n",
      "Iteration 187, loss = 115.06683385\n",
      "Iteration 188, loss = 114.03042864\n",
      "Iteration 189, loss = 113.00273930\n",
      "Iteration 190, loss = 111.98374752\n",
      "Iteration 191, loss = 110.97343361\n",
      "Iteration 192, loss = 109.97177651\n",
      "Iteration 193, loss = 108.97875383\n",
      "Iteration 194, loss = 107.99434193\n",
      "Iteration 195, loss = 107.01851588\n",
      "Iteration 196, loss = 106.05124958\n",
      "Iteration 197, loss = 105.09251570\n",
      "Iteration 198, loss = 104.14228580\n",
      "Iteration 199, loss = 103.20053031\n",
      "Iteration 200, loss = 102.26721860\n",
      "Iteration 201, loss = 101.34231896\n",
      "Iteration 202, loss = 100.42579871\n",
      "Iteration 203, loss = 99.51762414\n",
      "Iteration 204, loss = 98.61776064\n",
      "Iteration 205, loss = 97.72617264\n",
      "Iteration 206, loss = 96.84282372\n",
      "Iteration 207, loss = 95.96767657\n",
      "Iteration 208, loss = 95.10069308\n",
      "Iteration 209, loss = 94.24183433\n",
      "Iteration 210, loss = 93.39106065\n",
      "Iteration 211, loss = 92.54833162\n",
      "Iteration 212, loss = 91.71360610\n",
      "Iteration 213, loss = 90.88684229\n",
      "Iteration 214, loss = 90.06799774\n",
      "Iteration 215, loss = 89.25702934\n",
      "Iteration 216, loss = 88.45389342\n",
      "Iteration 217, loss = 87.65854571\n",
      "Iteration 218, loss = 86.87094140\n",
      "Iteration 219, loss = 86.09103517\n",
      "Iteration 220, loss = 85.31878117\n",
      "Iteration 221, loss = 84.55413311\n",
      "Iteration 222, loss = 83.79704423\n",
      "Iteration 223, loss = 83.04746735\n",
      "Iteration 224, loss = 82.30535489\n",
      "Iteration 225, loss = 81.57065887\n",
      "Iteration 226, loss = 80.84333096\n",
      "Iteration 227, loss = 80.12332251\n",
      "Iteration 228, loss = 79.41058452\n",
      "Iteration 229, loss = 78.70506770\n",
      "Iteration 230, loss = 78.00672251\n",
      "Iteration 231, loss = 77.31549913\n",
      "Iteration 232, loss = 76.63134748\n",
      "Iteration 233, loss = 75.95421731\n",
      "Iteration 234, loss = 75.28405812\n",
      "Iteration 235, loss = 74.62081926\n",
      "Iteration 236, loss = 73.96444990\n",
      "Iteration 237, loss = 73.31489905\n",
      "Iteration 238, loss = 72.67211561\n",
      "Iteration 239, loss = 72.03604834\n",
      "Iteration 240, loss = 71.40664591\n",
      "Iteration 241, loss = 70.78385691\n",
      "Iteration 242, loss = 70.16762985\n",
      "Iteration 243, loss = 69.55791317\n",
      "Iteration 244, loss = 68.95465530\n",
      "Iteration 245, loss = 68.35780462\n",
      "Iteration 246, loss = 67.76730949\n",
      "Iteration 247, loss = 67.18311829\n",
      "Iteration 248, loss = 66.60517940\n",
      "Iteration 249, loss = 66.03344120\n",
      "Iteration 250, loss = 65.46785215\n",
      "Iteration 251, loss = 64.90836072\n",
      "Iteration 252, loss = 64.35491545\n",
      "Iteration 253, loss = 63.80746496\n",
      "Iteration 254, loss = 63.26595793\n",
      "Iteration 255, loss = 62.73034315\n",
      "Iteration 256, loss = 62.20056950\n",
      "Iteration 257, loss = 61.67658596\n",
      "Iteration 258, loss = 61.15834166\n",
      "Iteration 259, loss = 60.64578582\n",
      "Iteration 260, loss = 60.13886783\n",
      "Iteration 261, loss = 59.63753721\n",
      "Iteration 262, loss = 59.14174365\n",
      "Iteration 263, loss = 58.65143697\n",
      "Iteration 264, loss = 58.16656719\n",
      "Iteration 265, loss = 57.68708451\n",
      "Iteration 266, loss = 57.21293930\n",
      "Iteration 267, loss = 56.74408212\n",
      "Iteration 268, loss = 56.28046374\n",
      "Iteration 269, loss = 55.82203514\n",
      "Iteration 270, loss = 55.36874750\n",
      "Iteration 271, loss = 54.92055222\n",
      "Iteration 272, loss = 54.47740093\n",
      "Iteration 273, loss = 54.03924548\n",
      "Iteration 274, loss = 53.60603797\n",
      "Iteration 275, loss = 53.17773071\n",
      "Iteration 276, loss = 52.75427630\n",
      "Iteration 277, loss = 52.33562755\n",
      "Iteration 278, loss = 51.92173754\n",
      "Iteration 279, loss = 51.51255961\n",
      "Iteration 280, loss = 51.10804736\n",
      "Iteration 281, loss = 50.70815466\n",
      "Iteration 282, loss = 50.31283564\n",
      "Iteration 283, loss = 49.92204471\n",
      "Iteration 284, loss = 49.53573656\n",
      "Iteration 285, loss = 49.15386617\n",
      "Iteration 286, loss = 48.77638877\n",
      "Iteration 287, loss = 48.40325993\n",
      "Iteration 288, loss = 48.03443545\n",
      "Iteration 289, loss = 47.66987146\n",
      "Iteration 290, loss = 47.30952438\n",
      "Iteration 291, loss = 46.95335092\n",
      "Iteration 292, loss = 46.60130809\n",
      "Iteration 293, loss = 46.25335319\n",
      "Iteration 294, loss = 45.90944385\n",
      "Iteration 295, loss = 45.56953797\n",
      "Iteration 296, loss = 45.23359379\n",
      "Iteration 297, loss = 44.90156984\n",
      "Iteration 298, loss = 44.57342495\n",
      "Iteration 299, loss = 44.24911827\n",
      "Iteration 300, loss = 43.92860926\n",
      "Iteration 301, loss = 43.61185770\n",
      "Iteration 302, loss = 43.29882367\n",
      "Iteration 303, loss = 42.98946756\n",
      "Iteration 304, loss = 42.68375009\n",
      "Iteration 305, loss = 42.38163229\n",
      "Iteration 306, loss = 42.08307549\n",
      "Iteration 307, loss = 41.78804136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 308, loss = 41.49649186\n",
      "Iteration 309, loss = 41.20838929\n",
      "Iteration 310, loss = 40.92369623\n",
      "Iteration 311, loss = 40.64237562\n",
      "Iteration 312, loss = 40.36439069\n",
      "Iteration 313, loss = 40.08970498\n",
      "Iteration 314, loss = 39.81828237\n",
      "Iteration 315, loss = 39.55008702\n",
      "Iteration 316, loss = 39.28508342\n",
      "Iteration 317, loss = 39.02323640\n",
      "Iteration 318, loss = 38.76451105\n",
      "Iteration 319, loss = 38.50887282\n",
      "Iteration 320, loss = 38.25628744\n",
      "Iteration 321, loss = 38.00672098\n",
      "Iteration 322, loss = 37.76013978\n",
      "Iteration 323, loss = 37.51651051\n",
      "Iteration 324, loss = 37.27580017\n",
      "Iteration 325, loss = 37.03797603\n",
      "Iteration 326, loss = 36.80300569\n",
      "Iteration 327, loss = 36.57085703\n",
      "Iteration 328, loss = 36.34149826\n",
      "Iteration 329, loss = 36.11489788\n",
      "Iteration 330, loss = 35.89102470\n",
      "Iteration 331, loss = 35.66984780\n",
      "Iteration 332, loss = 35.45133659\n",
      "Iteration 333, loss = 35.23546076\n",
      "Iteration 334, loss = 35.02219031\n",
      "Iteration 335, loss = 34.81149552\n",
      "Iteration 336, loss = 34.60334697\n",
      "Iteration 337, loss = 34.39771552\n",
      "Iteration 338, loss = 34.19457232\n",
      "Iteration 339, loss = 33.99388883\n",
      "Iteration 340, loss = 33.79563677\n",
      "Iteration 341, loss = 33.59978815\n",
      "Iteration 342, loss = 33.40631527\n",
      "Iteration 343, loss = 33.21519071\n",
      "Iteration 344, loss = 33.02638732\n",
      "Iteration 345, loss = 32.83987822\n",
      "Iteration 346, loss = 32.65563684\n",
      "Iteration 347, loss = 32.47363684\n",
      "Iteration 348, loss = 32.29385219\n",
      "Iteration 349, loss = 32.11625710\n",
      "Iteration 350, loss = 31.94082605\n",
      "Iteration 351, loss = 31.76753382\n",
      "Iteration 352, loss = 31.59635541\n",
      "Iteration 353, loss = 31.42726611\n",
      "Iteration 354, loss = 31.26024147\n",
      "Iteration 355, loss = 31.09525727\n",
      "Iteration 356, loss = 30.93228958\n",
      "Iteration 357, loss = 30.77131471\n",
      "Iteration 358, loss = 30.61230923\n",
      "Iteration 359, loss = 30.45524994\n",
      "Iteration 360, loss = 30.30011391\n",
      "Iteration 361, loss = 30.14687845\n",
      "Iteration 362, loss = 29.99552111\n",
      "Iteration 363, loss = 29.84601969\n",
      "Iteration 364, loss = 29.69835223\n",
      "Iteration 365, loss = 29.55249700\n",
      "Iteration 366, loss = 29.40843251\n",
      "Iteration 367, loss = 29.26613751\n",
      "Iteration 368, loss = 29.12559097\n",
      "Iteration 369, loss = 28.98677212\n",
      "Iteration 370, loss = 28.84966038\n",
      "Iteration 371, loss = 28.71423543\n",
      "Iteration 372, loss = 28.58047714\n",
      "Iteration 373, loss = 28.44836564\n",
      "Iteration 374, loss = 28.31788126\n",
      "Iteration 375, loss = 28.18900454\n",
      "Iteration 376, loss = 28.06171626\n",
      "Iteration 377, loss = 27.93599739\n",
      "Iteration 378, loss = 27.81182914\n",
      "Iteration 379, loss = 27.68919291\n",
      "Iteration 380, loss = 27.56807031\n",
      "Iteration 381, loss = 27.44844317\n",
      "Iteration 382, loss = 27.33029351\n",
      "Iteration 383, loss = 27.21360357\n",
      "Iteration 384, loss = 27.09835577\n",
      "Iteration 385, loss = 26.98453274\n",
      "Iteration 386, loss = 26.87211731\n",
      "Iteration 387, loss = 26.76109251\n",
      "Iteration 388, loss = 26.65144154\n",
      "Iteration 389, loss = 26.54314782\n",
      "Iteration 390, loss = 26.43619494\n",
      "Iteration 391, loss = 26.33056669\n",
      "Iteration 392, loss = 26.22624703\n",
      "Iteration 393, loss = 26.12322013\n",
      "Iteration 394, loss = 26.02147032\n",
      "Iteration 395, loss = 25.92098211\n",
      "Iteration 396, loss = 25.82174020\n",
      "Iteration 397, loss = 25.72372947\n",
      "Iteration 398, loss = 25.62693495\n",
      "Iteration 399, loss = 25.53134188\n",
      "Iteration 400, loss = 25.43693564\n",
      "Iteration 401, loss = 25.34370179\n",
      "Iteration 402, loss = 25.25162607\n",
      "Iteration 403, loss = 25.16069437\n",
      "Iteration 404, loss = 25.07089275\n",
      "Iteration 405, loss = 24.98220742\n",
      "Iteration 406, loss = 24.89462479\n",
      "Iteration 407, loss = 24.80813138\n",
      "Iteration 408, loss = 24.72271390\n",
      "Iteration 409, loss = 24.63835921\n",
      "Iteration 410, loss = 24.55505431\n",
      "Iteration 411, loss = 24.47278637\n",
      "Iteration 412, loss = 24.39154270\n",
      "Iteration 413, loss = 24.31131077\n",
      "Iteration 414, loss = 24.23207819\n",
      "Iteration 415, loss = 24.15383272\n",
      "Iteration 416, loss = 24.07656225\n",
      "Iteration 417, loss = 24.00025484\n",
      "Iteration 418, loss = 23.92489868\n",
      "Iteration 419, loss = 23.85048208\n",
      "Iteration 420, loss = 23.77699351\n",
      "Iteration 421, loss = 23.70442159\n",
      "Iteration 422, loss = 23.63275504\n",
      "Iteration 423, loss = 23.56198273\n",
      "Iteration 424, loss = 23.49209368\n",
      "Iteration 425, loss = 23.42307701\n",
      "Iteration 426, loss = 23.35492200\n",
      "Iteration 427, loss = 23.28761803\n",
      "Iteration 428, loss = 23.22115462\n",
      "Iteration 429, loss = 23.15552143\n",
      "Iteration 430, loss = 23.09070821\n",
      "Iteration 431, loss = 23.02670486\n",
      "Iteration 432, loss = 22.96350140\n",
      "Iteration 433, loss = 22.90108795\n",
      "Iteration 434, loss = 22.83945477\n",
      "Iteration 435, loss = 22.77859222\n",
      "Iteration 436, loss = 22.71849079\n",
      "Iteration 437, loss = 22.65914108\n",
      "Iteration 438, loss = 22.60053379\n",
      "Iteration 439, loss = 22.54265976\n",
      "Iteration 440, loss = 22.48550991\n",
      "Iteration 441, loss = 22.42907529\n",
      "Iteration 442, loss = 22.37334705\n",
      "Iteration 443, loss = 22.31831645\n",
      "Iteration 444, loss = 22.26397484\n",
      "Iteration 445, loss = 22.21031371\n",
      "Iteration 446, loss = 22.15732463\n",
      "Iteration 447, loss = 22.10499925\n",
      "Iteration 448, loss = 22.05332937\n",
      "Iteration 449, loss = 22.00230686\n",
      "Iteration 450, loss = 21.95192368\n",
      "Iteration 451, loss = 21.90217191\n",
      "Iteration 452, loss = 21.85304371\n",
      "Iteration 453, loss = 21.80453135\n",
      "Iteration 454, loss = 21.75662719\n",
      "Iteration 455, loss = 21.70932366\n",
      "Iteration 456, loss = 21.66261332\n",
      "Iteration 457, loss = 21.61648880\n",
      "Iteration 458, loss = 21.57094280\n",
      "Iteration 459, loss = 21.52596816\n",
      "Iteration 460, loss = 21.48155776\n",
      "Iteration 461, loss = 21.43770458\n",
      "Iteration 462, loss = 21.39440171\n",
      "Iteration 463, loss = 21.35164228\n",
      "Iteration 464, loss = 21.30941955\n",
      "Iteration 465, loss = 21.26772684\n",
      "Iteration 466, loss = 21.22655754\n",
      "Iteration 467, loss = 21.18590513\n",
      "Iteration 468, loss = 21.14576319\n",
      "Iteration 469, loss = 21.10612536\n",
      "Iteration 470, loss = 21.06698535\n",
      "Iteration 471, loss = 21.02833697\n",
      "Iteration 472, loss = 20.99017408\n",
      "Iteration 473, loss = 20.95249063\n",
      "Iteration 474, loss = 20.91528065\n",
      "Iteration 475, loss = 20.87853823\n",
      "Iteration 476, loss = 20.84225754\n",
      "Iteration 477, loss = 20.80643283\n",
      "Iteration 478, loss = 20.77105840\n",
      "Iteration 479, loss = 20.73612863\n",
      "Iteration 480, loss = 20.70163797\n",
      "Iteration 481, loss = 20.66758094\n",
      "Iteration 482, loss = 20.63395214\n",
      "Iteration 483, loss = 20.60074620\n",
      "Iteration 484, loss = 20.56795785\n",
      "Iteration 485, loss = 20.53558188\n",
      "Iteration 486, loss = 20.50361312\n",
      "Iteration 487, loss = 20.47204650\n",
      "Iteration 488, loss = 20.44087698\n",
      "Iteration 489, loss = 20.41009961\n",
      "Iteration 490, loss = 20.37970947\n",
      "Iteration 491, loss = 20.34970173\n",
      "Iteration 492, loss = 20.32007161\n",
      "Iteration 493, loss = 20.29081438\n",
      "Iteration 494, loss = 20.26192537\n",
      "Iteration 495, loss = 20.23339999\n",
      "Iteration 496, loss = 20.20523368\n",
      "Iteration 497, loss = 20.17742194\n",
      "Iteration 498, loss = 20.14996035\n",
      "Iteration 499, loss = 20.12284451\n",
      "Iteration 500, loss = 20.09607009\n",
      "Iteration 501, loss = 20.06963283\n",
      "Iteration 502, loss = 20.04352850\n",
      "Iteration 503, loss = 20.01775292\n",
      "Iteration 504, loss = 19.99230199\n",
      "Iteration 505, loss = 19.96717163\n",
      "Iteration 506, loss = 19.94235783\n",
      "Iteration 507, loss = 19.91785661\n",
      "Iteration 508, loss = 19.89366407\n",
      "Iteration 509, loss = 19.86977634\n",
      "Iteration 510, loss = 19.84618958\n",
      "Iteration 511, loss = 19.82290003\n",
      "Iteration 512, loss = 19.79990396\n",
      "Iteration 513, loss = 19.77719770\n",
      "Iteration 514, loss = 19.75477760\n",
      "Iteration 515, loss = 19.73264008\n",
      "Iteration 516, loss = 19.71078158\n",
      "Iteration 517, loss = 19.68919862\n",
      "Iteration 518, loss = 19.66788774\n",
      "Iteration 519, loss = 19.64684551\n",
      "Iteration 520, loss = 19.62606857\n",
      "Iteration 521, loss = 19.60555359\n",
      "Iteration 522, loss = 19.58529727\n",
      "Iteration 523, loss = 19.56529638\n",
      "Iteration 524, loss = 19.54554770\n",
      "Iteration 525, loss = 19.52604808\n",
      "Iteration 526, loss = 19.50679437\n",
      "Iteration 527, loss = 19.48778349\n",
      "Iteration 528, loss = 19.46901239\n",
      "Iteration 529, loss = 19.45047806\n",
      "Iteration 530, loss = 19.43217753\n",
      "Iteration 531, loss = 19.41410785\n",
      "Iteration 532, loss = 19.39626613\n",
      "Iteration 533, loss = 19.37864950\n",
      "Iteration 534, loss = 19.36125513\n",
      "Iteration 535, loss = 19.34408024\n",
      "Iteration 536, loss = 19.32712205\n",
      "Iteration 537, loss = 19.31037785\n",
      "Iteration 538, loss = 19.29384494\n",
      "Iteration 539, loss = 19.27752067\n",
      "Iteration 540, loss = 19.26140242\n",
      "Iteration 541, loss = 19.24548759\n",
      "Iteration 542, loss = 19.22977363\n",
      "Iteration 543, loss = 19.21425800\n",
      "Iteration 544, loss = 19.19893823\n",
      "Iteration 545, loss = 19.18381183\n",
      "Iteration 546, loss = 19.16887638\n",
      "Iteration 547, loss = 19.15412948\n",
      "Iteration 548, loss = 19.13956875\n",
      "Iteration 549, loss = 19.12519185\n",
      "Iteration 550, loss = 19.11099647\n",
      "Iteration 551, loss = 19.09698032\n",
      "Iteration 552, loss = 19.08314115\n",
      "Iteration 553, loss = 19.06947673\n",
      "Iteration 554, loss = 19.05598485\n",
      "Iteration 555, loss = 19.04266336\n",
      "Iteration 556, loss = 19.02951009\n",
      "Iteration 557, loss = 19.01652294\n",
      "Iteration 558, loss = 19.00369981\n",
      "Iteration 559, loss = 18.99103864\n",
      "Iteration 560, loss = 18.97853739\n",
      "Iteration 561, loss = 18.96619404\n",
      "Iteration 562, loss = 18.95400660\n",
      "Iteration 563, loss = 18.94197311\n",
      "Iteration 564, loss = 18.93009164\n",
      "Iteration 565, loss = 18.91836026\n",
      "Iteration 566, loss = 18.90677708\n",
      "Iteration 567, loss = 18.89534024\n",
      "Iteration 568, loss = 18.88404790\n",
      "Iteration 569, loss = 18.87289823\n",
      "Iteration 570, loss = 18.86188943\n",
      "Iteration 571, loss = 18.85101974\n",
      "Iteration 572, loss = 18.84028739\n",
      "Iteration 573, loss = 18.82969065\n",
      "Iteration 574, loss = 18.81922783\n",
      "Iteration 575, loss = 18.80889722\n",
      "Iteration 576, loss = 18.79869716\n",
      "Iteration 577, loss = 18.78862601\n",
      "Iteration 578, loss = 18.77868214\n",
      "Iteration 579, loss = 18.76886394\n",
      "Iteration 580, loss = 18.75916984\n",
      "Iteration 581, loss = 18.74959826\n",
      "Iteration 582, loss = 18.74014766\n",
      "Iteration 583, loss = 18.73081652\n",
      "Iteration 584, loss = 18.72160333\n",
      "Iteration 585, loss = 18.71250660\n",
      "Iteration 586, loss = 18.70352486\n",
      "Iteration 587, loss = 18.69465666\n",
      "Iteration 588, loss = 18.68590058\n",
      "Iteration 589, loss = 18.67725518\n",
      "Iteration 590, loss = 18.66871909\n",
      "Iteration 591, loss = 18.66029092\n",
      "Iteration 592, loss = 18.65196931\n",
      "Iteration 593, loss = 18.64375291\n",
      "Iteration 594, loss = 18.63564039\n",
      "Iteration 595, loss = 18.62763046\n",
      "Iteration 596, loss = 18.61972180\n",
      "Iteration 597, loss = 18.61191315\n",
      "Iteration 598, loss = 18.60420324\n",
      "Iteration 599, loss = 18.59659082\n",
      "Iteration 600, loss = 18.58907467\n",
      "Iteration 601, loss = 18.58165356\n",
      "Iteration 602, loss = 18.57432631\n",
      "Iteration 603, loss = 18.56709172\n",
      "Iteration 604, loss = 18.55994863\n",
      "Iteration 605, loss = 18.55289587\n",
      "Iteration 606, loss = 18.54593232\n",
      "Iteration 607, loss = 18.53905684\n",
      "Iteration 608, loss = 18.53226832\n",
      "Iteration 609, loss = 18.52556566\n",
      "Iteration 610, loss = 18.51894778\n",
      "Iteration 611, loss = 18.51241362\n",
      "Iteration 612, loss = 18.50596210\n",
      "Iteration 613, loss = 18.49959219\n",
      "Iteration 614, loss = 18.49330286\n",
      "Iteration 615, loss = 18.48709309\n",
      "Iteration 616, loss = 18.48096187\n",
      "Iteration 617, loss = 18.47490822\n",
      "Iteration 618, loss = 18.46893115\n",
      "Iteration 619, loss = 18.46302969\n",
      "Iteration 620, loss = 18.45720290\n",
      "Iteration 621, loss = 18.45144982\n",
      "Iteration 622, loss = 18.44576953\n",
      "Iteration 623, loss = 18.44016111\n",
      "Iteration 624, loss = 18.43462364\n",
      "Iteration 625, loss = 18.42915624\n",
      "Iteration 626, loss = 18.42375801\n",
      "Iteration 627, loss = 18.41842809\n",
      "Iteration 628, loss = 18.41316561\n",
      "Iteration 629, loss = 18.40796971\n",
      "Iteration 630, loss = 18.40283956\n",
      "Iteration 631, loss = 18.39777433\n",
      "Iteration 632, loss = 18.39277320\n",
      "Iteration 633, loss = 18.38783534\n",
      "Iteration 634, loss = 18.38295998\n",
      "Iteration 635, loss = 18.37814631\n",
      "Iteration 636, loss = 18.37339356\n",
      "Iteration 637, loss = 18.36870095\n",
      "Iteration 638, loss = 18.36406773\n",
      "Iteration 639, loss = 18.35949314\n",
      "Iteration 640, loss = 18.35497645\n",
      "Iteration 641, loss = 18.35051692\n",
      "Iteration 642, loss = 18.34611383\n",
      "Iteration 643, loss = 18.34176647\n",
      "Iteration 644, loss = 18.33747413\n",
      "Iteration 645, loss = 18.33323612\n",
      "Iteration 646, loss = 18.32905175\n",
      "Iteration 647, loss = 18.32492033\n",
      "Iteration 648, loss = 18.32084121\n",
      "Iteration 649, loss = 18.31681372\n",
      "Iteration 650, loss = 18.31283721\n",
      "Iteration 651, loss = 18.30891103\n",
      "Iteration 652, loss = 18.30503455\n",
      "Iteration 653, loss = 18.30120714\n",
      "Iteration 654, loss = 18.29742817\n",
      "Iteration 655, loss = 18.29369704\n",
      "Iteration 656, loss = 18.29001315\n",
      "Iteration 657, loss = 18.28637588\n",
      "Iteration 658, loss = 18.28278466\n",
      "Iteration 659, loss = 18.27923890\n",
      "Iteration 660, loss = 18.27573802\n",
      "Iteration 661, loss = 18.27228147\n",
      "Iteration 662, loss = 18.26886867\n",
      "Iteration 663, loss = 18.26549907\n",
      "Iteration 664, loss = 18.26217214\n",
      "Iteration 665, loss = 18.25888732\n",
      "Iteration 666, loss = 18.25564409\n",
      "Iteration 667, loss = 18.25244192\n",
      "Iteration 668, loss = 18.24928029\n",
      "Iteration 669, loss = 18.24615868\n",
      "Iteration 670, loss = 18.24307660\n",
      "Iteration 671, loss = 18.24003354\n",
      "Iteration 672, loss = 18.23702901\n",
      "Iteration 673, loss = 18.23406251\n",
      "Iteration 674, loss = 18.23113358\n",
      "Iteration 675, loss = 18.22824173\n",
      "Iteration 676, loss = 18.22538649\n",
      "Iteration 677, loss = 18.22256740\n",
      "Iteration 678, loss = 18.21978401\n",
      "Iteration 679, loss = 18.21703586\n",
      "Iteration 680, loss = 18.21432251\n",
      "Iteration 681, loss = 18.21164351\n",
      "Iteration 682, loss = 18.20899843\n",
      "Iteration 683, loss = 18.20638685\n",
      "Iteration 684, loss = 18.20380833\n",
      "Iteration 685, loss = 18.20126247\n",
      "Iteration 686, loss = 18.19874884\n",
      "Iteration 687, loss = 18.19626704\n",
      "Iteration 688, loss = 18.19381666\n",
      "Iteration 689, loss = 18.19139731\n",
      "Iteration 690, loss = 18.18900860\n",
      "Iteration 691, loss = 18.18665014\n",
      "Iteration 692, loss = 18.18432154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 693, loss = 18.18202242\n",
      "Iteration 694, loss = 18.17975242\n",
      "Iteration 695, loss = 18.17751117\n",
      "Iteration 696, loss = 18.17529830\n",
      "Iteration 697, loss = 18.17311344\n",
      "Iteration 698, loss = 18.17095626\n",
      "Iteration 699, loss = 18.16882639\n",
      "Iteration 700, loss = 18.16672350\n",
      "Iteration 701, loss = 18.16464723\n",
      "Iteration 702, loss = 18.16259726\n",
      "Iteration 703, loss = 18.16057325\n",
      "Iteration 704, loss = 18.15857487\n",
      "Iteration 705, loss = 18.15660179\n",
      "Iteration 706, loss = 18.15465370\n",
      "Iteration 707, loss = 18.15273028\n",
      "Iteration 708, loss = 18.15083122\n",
      "Iteration 709, loss = 18.14895621\n",
      "Iteration 710, loss = 18.14710495\n",
      "Iteration 711, loss = 18.14527712\n",
      "Iteration 712, loss = 18.14347245\n",
      "Iteration 713, loss = 18.14169063\n",
      "Iteration 714, loss = 18.13993137\n",
      "Iteration 715, loss = 18.13819440\n",
      "Iteration 716, loss = 18.13647942\n",
      "Iteration 717, loss = 18.13478616\n",
      "Iteration 718, loss = 18.13311434\n",
      "Iteration 719, loss = 18.13146369\n",
      "Iteration 720, loss = 18.12983395\n",
      "Iteration 721, loss = 18.12822485\n",
      "Iteration 722, loss = 18.12663612\n",
      "Iteration 723, loss = 18.12506752\n",
      "Iteration 724, loss = 18.12351878\n",
      "Iteration 725, loss = 18.12198965\n",
      "Iteration 726, loss = 18.12047989\n",
      "Iteration 727, loss = 18.11898925\n",
      "Iteration 728, loss = 18.11751748\n",
      "Iteration 729, loss = 18.11606436\n",
      "Iteration 730, loss = 18.11462963\n",
      "Iteration 731, loss = 18.11321308\n",
      "Iteration 732, loss = 18.11181446\n",
      "Iteration 733, loss = 18.11043356\n",
      "Iteration 734, loss = 18.10907014\n",
      "Iteration 735, loss = 18.10772399\n",
      "Iteration 736, loss = 18.10639489\n",
      "Iteration 737, loss = 18.10508262\n",
      "Iteration 738, loss = 18.10378697\n",
      "Iteration 739, loss = 18.10250772\n",
      "Iteration 740, loss = 18.10124467\n",
      "Iteration 741, loss = 18.09999762\n",
      "Iteration 742, loss = 18.09876636\n",
      "Iteration 743, loss = 18.09755070\n",
      "Iteration 744, loss = 18.09635042\n",
      "Iteration 745, loss = 18.09516535\n",
      "Iteration 746, loss = 18.09399528\n",
      "Iteration 747, loss = 18.09284003\n",
      "Iteration 748, loss = 18.09169941\n",
      "Iteration 749, loss = 18.09057323\n",
      "Iteration 750, loss = 18.08946131\n",
      "Iteration 751, loss = 18.08836348\n",
      "Iteration 752, loss = 18.08727954\n",
      "Iteration 753, loss = 18.08620933\n",
      "Iteration 754, loss = 18.08515267\n",
      "Iteration 755, loss = 18.08410939\n",
      "Iteration 756, loss = 18.08307932\n",
      "Iteration 757, loss = 18.08206229\n",
      "Iteration 758, loss = 18.08105814\n",
      "Iteration 759, loss = 18.08006671\n",
      "Iteration 760, loss = 18.07908782\n",
      "Iteration 761, loss = 18.07812134\n",
      "Iteration 762, loss = 18.07716708\n",
      "Iteration 763, loss = 18.07622491\n",
      "Iteration 764, loss = 18.07529467\n",
      "Iteration 765, loss = 18.07437621\n",
      "Iteration 766, loss = 18.07346937\n",
      "Iteration 767, loss = 18.07257402\n",
      "Iteration 768, loss = 18.07169000\n",
      "Iteration 769, loss = 18.07081717\n",
      "Iteration 770, loss = 18.06995540\n",
      "Iteration 771, loss = 18.06910453\n",
      "Iteration 772, loss = 18.06826443\n",
      "Iteration 773, loss = 18.06743497\n",
      "Iteration 774, loss = 18.06661601\n",
      "Iteration 775, loss = 18.06580741\n",
      "Iteration 776, loss = 18.06500906\n",
      "Iteration 777, loss = 18.06422080\n",
      "Iteration 778, loss = 18.06344253\n",
      "Iteration 779, loss = 18.06267410\n",
      "Iteration 780, loss = 18.06191541\n",
      "Iteration 781, loss = 18.06116631\n",
      "Iteration 782, loss = 18.06042670\n",
      "Iteration 783, loss = 18.05969644\n",
      "Iteration 784, loss = 18.05897543\n",
      "Iteration 785, loss = 18.05826355\n",
      "Iteration 786, loss = 18.05756067\n",
      "Iteration 787, loss = 18.05686669\n",
      "Iteration 788, loss = 18.05618149\n",
      "Iteration 789, loss = 18.05550496\n",
      "Iteration 790, loss = 18.05483700\n",
      "Iteration 791, loss = 18.05417748\n",
      "Iteration 792, loss = 18.05352632\n",
      "Iteration 793, loss = 18.05288339\n",
      "Iteration 794, loss = 18.05224859\n",
      "Iteration 795, loss = 18.05162183\n",
      "Iteration 796, loss = 18.05100300\n",
      "Iteration 797, loss = 18.05039199\n",
      "Iteration 798, loss = 18.04978872\n",
      "Iteration 799, loss = 18.04919308\n",
      "Iteration 800, loss = 18.04860497\n",
      "Iteration 801, loss = 18.04802430\n",
      "Iteration 802, loss = 18.04745098\n",
      "Iteration 803, loss = 18.04688491\n",
      "Iteration 804, loss = 18.04632600\n",
      "Iteration 805, loss = 18.04577416\n",
      "Iteration 806, loss = 18.04522930\n",
      "Iteration 807, loss = 18.04469133\n",
      "Iteration 808, loss = 18.04416016\n",
      "Iteration 809, loss = 18.04363571\n",
      "Iteration 810, loss = 18.04311789\n",
      "Iteration 811, loss = 18.04260662\n",
      "Iteration 812, loss = 18.04210181\n",
      "Iteration 813, loss = 18.04160339\n",
      "Iteration 814, loss = 18.04111127\n",
      "Iteration 815, loss = 18.04062537\n",
      "Iteration 816, loss = 18.04014561\n",
      "Iteration 817, loss = 18.03967192\n",
      "Iteration 818, loss = 18.03920421\n",
      "Iteration 819, loss = 18.03874242\n",
      "Iteration 820, loss = 18.03828646\n",
      "Iteration 821, loss = 18.03783627\n",
      "Iteration 822, loss = 18.03739176\n",
      "Iteration 823, loss = 18.03695287\n",
      "Iteration 824, loss = 18.03651953\n",
      "Iteration 825, loss = 18.03609166\n",
      "Iteration 826, loss = 18.03566920\n",
      "Iteration 827, loss = 18.03525208\n",
      "Iteration 828, loss = 18.03484022\n",
      "Iteration 829, loss = 18.03443357\n",
      "Iteration 830, loss = 18.03403206\n",
      "Iteration 831, loss = 18.03363561\n",
      "Iteration 832, loss = 18.03324417\n",
      "Iteration 833, loss = 18.03285768\n",
      "Iteration 834, loss = 18.03247606\n",
      "Iteration 835, loss = 18.03209927\n",
      "Iteration 836, loss = 18.03172723\n",
      "Iteration 837, loss = 18.03135989\n",
      "Iteration 838, loss = 18.03099718\n",
      "Iteration 839, loss = 18.03063906\n",
      "Iteration 840, loss = 18.03028545\n",
      "Iteration 841, loss = 18.02993631\n",
      "Iteration 842, loss = 18.02959157\n",
      "Iteration 843, loss = 18.02925118\n",
      "Iteration 844, loss = 18.02891508\n",
      "Iteration 845, loss = 18.02858323\n",
      "Iteration 846, loss = 18.02825556\n",
      "Iteration 847, loss = 18.02793203\n",
      "Iteration 848, loss = 18.02761257\n",
      "Iteration 849, loss = 18.02729714\n",
      "Iteration 850, loss = 18.02698569\n",
      "Iteration 851, loss = 18.02667817\n",
      "Iteration 852, loss = 18.02637452\n",
      "Iteration 853, loss = 18.02607470\n",
      "Iteration 854, loss = 18.02577866\n",
      "Iteration 855, loss = 18.02548635\n",
      "Iteration 856, loss = 18.02519773\n",
      "Iteration 857, loss = 18.02491273\n",
      "Iteration 858, loss = 18.02463133\n",
      "Iteration 859, loss = 18.02435348\n",
      "Iteration 860, loss = 18.02407912\n",
      "Iteration 861, loss = 18.02380822\n",
      "Iteration 862, loss = 18.02354072\n",
      "Iteration 863, loss = 18.02327660\n",
      "Iteration 864, loss = 18.02301580\n",
      "Iteration 865, loss = 18.02275828\n",
      "Iteration 866, loss = 18.02250400\n",
      "Iteration 867, loss = 18.02225292\n",
      "Iteration 868, loss = 18.02200499\n",
      "Iteration 869, loss = 18.02176019\n",
      "Iteration 870, loss = 18.02151846\n",
      "Iteration 871, loss = 18.02127978\n",
      "Iteration 872, loss = 18.02104409\n",
      "Iteration 873, loss = 18.02081136\n",
      "Iteration 874, loss = 18.02058156\n",
      "Iteration 875, loss = 18.02035465\n",
      "Iteration 876, loss = 18.02013058\n",
      "Iteration 877, loss = 18.01990933\n",
      "Iteration 878, loss = 18.01969086\n",
      "Iteration 879, loss = 18.01947513\n",
      "Iteration 880, loss = 18.01926211\n",
      "Iteration 881, loss = 18.01905176\n",
      "Iteration 882, loss = 18.01884405\n",
      "Iteration 883, loss = 18.01863894\n",
      "Iteration 884, loss = 18.01843640\n",
      "Iteration 885, loss = 18.01823641\n",
      "Iteration 886, loss = 18.01803892\n",
      "Iteration 887, loss = 18.01784390\n",
      "Iteration 888, loss = 18.01765133\n",
      "Iteration 889, loss = 18.01746117\n",
      "Iteration 890, loss = 18.01727339\n",
      "Iteration 891, loss = 18.01708796\n",
      "Iteration 892, loss = 18.01690485\n",
      "Iteration 893, loss = 18.01672403\n",
      "Iteration 894, loss = 18.01654548\n",
      "Iteration 895, loss = 18.01636915\n",
      "Iteration 896, loss = 18.01619503\n",
      "Iteration 897, loss = 18.01602309\n",
      "Iteration 898, loss = 18.01585329\n",
      "Iteration 899, loss = 18.01568561\n",
      "Iteration 900, loss = 18.01552002\n",
      "Iteration 901, loss = 18.01535651\n",
      "Iteration 902, loss = 18.01519503\n",
      "Iteration 903, loss = 18.01503556\n",
      "Iteration 904, loss = 18.01487808\n",
      "Iteration 905, loss = 18.01472257\n",
      "Iteration 906, loss = 18.01456899\n",
      "Iteration 907, loss = 18.01441732\n",
      "Iteration 908, loss = 18.01426754\n",
      "Iteration 909, loss = 18.01411963\n",
      "Iteration 910, loss = 18.01397355\n",
      "Iteration 911, loss = 18.01382929\n",
      "Iteration 912, loss = 18.01368682\n",
      "Iteration 913, loss = 18.01354612\n",
      "Iteration 914, loss = 18.01340717\n",
      "Iteration 915, loss = 18.01326994\n",
      "Iteration 916, loss = 18.01313441\n",
      "Iteration 917, loss = 18.01300056\n",
      "Iteration 918, loss = 18.01286837\n",
      "Iteration 919, loss = 18.01273782\n",
      "Iteration 920, loss = 18.01260888\n",
      "Iteration 921, loss = 18.01248154\n",
      "Iteration 922, loss = 18.01235577\n",
      "Iteration 923, loss = 18.01223156\n",
      "Iteration 924, loss = 18.01210888\n",
      "Iteration 925, loss = 18.01198771\n",
      "Iteration 926, loss = 18.01186804\n",
      "Iteration 927, loss = 18.01174985\n",
      "Iteration 928, loss = 18.01163311\n",
      "Iteration 929, loss = 18.01151780\n",
      "Iteration 930, loss = 18.01140392\n",
      "Iteration 931, loss = 18.01129143\n",
      "Iteration 932, loss = 18.01118033\n",
      "Iteration 933, loss = 18.01107059\n",
      "Iteration 934, loss = 18.01096219\n",
      "Iteration 935, loss = 18.01085513\n",
      "Iteration 936, loss = 18.01074937\n",
      "Iteration 937, loss = 18.01064491\n",
      "Iteration 938, loss = 18.01054173\n",
      "Iteration 939, loss = 18.01043981\n",
      "Iteration 940, loss = 18.01033913\n",
      "Iteration 941, loss = 18.01023968\n",
      "Iteration 942, loss = 18.01014144\n",
      "Iteration 943, loss = 18.01004440\n",
      "Iteration 944, loss = 18.00994854\n",
      "Iteration 945, loss = 18.00985384\n",
      "Iteration 946, loss = 18.00976029\n",
      "Iteration 947, loss = 18.00966788\n",
      "Iteration 948, loss = 18.00957659\n",
      "Iteration 949, loss = 18.00948640\n",
      "Iteration 950, loss = 18.00939730\n",
      "Iteration 951, loss = 18.00930928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000020\n",
      "Iteration 952, loss = 18.00922232\n",
      "Iteration 953, loss = 18.00914878\n",
      "Iteration 954, loss = 18.00908132\n",
      "Iteration 955, loss = 18.00901929\n",
      "Iteration 956, loss = 18.00896212\n",
      "Iteration 957, loss = 18.00890931\n",
      "Iteration 958, loss = 18.00886039\n",
      "Iteration 959, loss = 18.00881497\n",
      "Iteration 960, loss = 18.00877269\n",
      "Iteration 961, loss = 18.00873322\n",
      "Iteration 962, loss = 18.00869628\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000004\n",
      "Iteration 963, loss = 18.00866161\n",
      "Iteration 964, loss = 18.00863127\n",
      "Iteration 965, loss = 18.00860373\n",
      "Iteration 966, loss = 18.00857869\n",
      "Iteration 967, loss = 18.00855590\n",
      "Iteration 968, loss = 18.00853513\n",
      "Iteration 969, loss = 18.00851616\n",
      "Iteration 970, loss = 18.00849882\n",
      "Iteration 971, loss = 18.00848295\n",
      "Iteration 972, loss = 18.00846838\n",
      "Iteration 973, loss = 18.00845499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 974, loss = 18.00844266\n",
      "Iteration 975, loss = 18.00843172\n",
      "Iteration 976, loss = 18.00842183\n",
      "Iteration 977, loss = 18.00841288\n",
      "Iteration 978, loss = 18.00840477\n",
      "Iteration 979, loss = 18.00839741\n",
      "Iteration 980, loss = 18.00839074\n",
      "Iteration 981, loss = 18.00838468\n",
      "Iteration 982, loss = 18.00837917\n",
      "Iteration 983, loss = 18.00837415\n",
      "Iteration 984, loss = 18.00836959\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic', batch_size=40, hidden_layer_sizes=(2, 3, 5),\n",
       "             learning_rate='adaptive', learning_rate_init=0.0001, max_iter=2000,\n",
       "             random_state=1, solver='sgd', verbose=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =[[22,33],\n",
    "           [44,55]]\n",
    "\n",
    "y_test= [22,33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27.89271939, 27.89280956])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "y_pred=reg.predict(X_test)\n",
    "y_pred\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14958099526.163626"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ON DIABETES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "dataset = load_diabetes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "          0.01990842, -0.01764613],\n",
       "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "         -0.06832974, -0.09220405],\n",
       "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "          0.00286377, -0.02593034],\n",
       "        ...,\n",
       "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "         -0.04687948,  0.01549073],\n",
       "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "          0.04452837, -0.02593034],\n",
       "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "         -0.00421986,  0.00306441]]),\n",
       " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "        220.,  57.]),\n",
       " 'frame': None,\n",
       " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - age     age in years\\n      - sex\\n      - bmi     body mass index\\n      - bp      average blood pressure\\n      - s1      tc, total serum cholesterol\\n      - s2      ldl, low-density lipoproteins\\n      - s3      hdl, high-density lipoproteins\\n      - s4      tch, total cholesterol / HDL\\n      - s5      ltg, possibly log of serum triglycerides level\\n      - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)',\n",
       " 'feature_names': ['age',\n",
       "  'sex',\n",
       "  'bmi',\n",
       "  'bp',\n",
       "  's1',\n",
       "  's2',\n",
       "  's3',\n",
       "  's4',\n",
       "  's5',\n",
       "  's6'],\n",
       " 'data_filename': 'diabetes_data.csv.gz',\n",
       " 'target_filename': 'diabetes_target.csv.gz',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes=(2,3,5),\n",
    "                   verbose=2,\n",
    "                   activation=\"logistic\" ,\n",
    "                   batch_size=40,\n",
    "                   random_state=1, \n",
    "                   max_iter=2000,\n",
    "                   learning_rate =\"adaptive\",\n",
    "                   solver='sgd',\n",
    "                   learning_rate_init=0.0001)\n",
    "\n",
    "\n",
    "# 2 neurons and then 3 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 14479.66758089\n",
      "Iteration 2, loss = 14057.13392201\n",
      "Iteration 3, loss = 13430.17733340\n",
      "Iteration 4, loss = 12525.38694964\n",
      "Iteration 5, loss = 11383.26008983\n",
      "Iteration 6, loss = 10259.02785560\n",
      "Iteration 7, loss = 9248.02824397\n",
      "Iteration 8, loss = 8380.33100948\n",
      "Iteration 9, loss = 7589.26708029\n",
      "Iteration 10, loss = 6911.61141491\n",
      "Iteration 11, loss = 6344.28621329\n",
      "Iteration 12, loss = 5860.22685587\n",
      "Iteration 13, loss = 5437.08580684\n",
      "Iteration 14, loss = 5090.18235679\n",
      "Iteration 15, loss = 4767.06670163\n",
      "Iteration 16, loss = 4491.77793511\n",
      "Iteration 17, loss = 4267.75724302\n",
      "Iteration 18, loss = 4086.31343143\n",
      "Iteration 19, loss = 3926.03717911\n",
      "Iteration 20, loss = 3782.20055045\n",
      "Iteration 21, loss = 3651.39236308\n",
      "Iteration 22, loss = 3545.32655296\n",
      "Iteration 23, loss = 3448.59188925\n",
      "Iteration 24, loss = 3383.06722930\n",
      "Iteration 25, loss = 3331.92897232\n",
      "Iteration 26, loss = 3273.32487459\n",
      "Iteration 27, loss = 3223.17381816\n",
      "Iteration 28, loss = 3184.95953167\n",
      "Iteration 29, loss = 3155.51050602\n",
      "Iteration 30, loss = 3127.71791512\n",
      "Iteration 31, loss = 3106.28963518\n",
      "Iteration 32, loss = 3088.73523779\n",
      "Iteration 33, loss = 3071.67337095\n",
      "Iteration 34, loss = 3054.44152664\n",
      "Iteration 35, loss = 3039.28979110\n",
      "Iteration 36, loss = 3025.82641906\n",
      "Iteration 37, loss = 3014.63988056\n",
      "Iteration 38, loss = 3007.25174059\n",
      "Iteration 39, loss = 3001.40595612\n",
      "Iteration 40, loss = 2996.79659358\n",
      "Iteration 41, loss = 2994.35183327\n",
      "Iteration 42, loss = 2992.42540726\n",
      "Iteration 43, loss = 2989.76633928\n",
      "Iteration 44, loss = 2984.61341525\n",
      "Iteration 45, loss = 2981.74835798\n",
      "Iteration 46, loss = 2978.66269554\n",
      "Iteration 47, loss = 2977.21639613\n",
      "Iteration 48, loss = 2976.51734937\n",
      "Iteration 49, loss = 2975.95422247\n",
      "Iteration 50, loss = 2972.90456635\n",
      "Iteration 51, loss = 2972.07060684\n",
      "Iteration 52, loss = 2971.27235240\n",
      "Iteration 53, loss = 2970.16658889\n",
      "Iteration 54, loss = 2968.06618970\n",
      "Iteration 55, loss = 2967.48864382\n",
      "Iteration 56, loss = 2967.54519594\n",
      "Iteration 57, loss = 2966.43674756\n",
      "Iteration 58, loss = 2965.52687281\n",
      "Iteration 59, loss = 2965.70482872\n",
      "Iteration 60, loss = 2965.23133158\n",
      "Iteration 61, loss = 2965.37926359\n",
      "Iteration 62, loss = 2965.31745157\n",
      "Iteration 63, loss = 2965.12029438\n",
      "Iteration 64, loss = 2965.23828999\n",
      "Iteration 65, loss = 2964.98048820\n",
      "Iteration 66, loss = 2965.18120639\n",
      "Iteration 67, loss = 2965.31541787\n",
      "Iteration 68, loss = 2964.93485164\n",
      "Iteration 69, loss = 2965.21026927\n",
      "Iteration 70, loss = 2965.32566905\n",
      "Iteration 71, loss = 2965.28845208\n",
      "Iteration 72, loss = 2966.14600071\n",
      "Iteration 73, loss = 2965.46343891\n",
      "Iteration 74, loss = 2965.29774383\n",
      "Iteration 75, loss = 2965.25375113\n",
      "Iteration 76, loss = 2965.34354128\n",
      "Iteration 77, loss = 2965.31502616\n",
      "Iteration 78, loss = 2965.24497817\n",
      "Iteration 79, loss = 2965.24818405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000020\n",
      "Iteration 80, loss = 2965.40580645\n",
      "Iteration 81, loss = 2965.40147947\n",
      "Iteration 82, loss = 2965.39456781\n",
      "Iteration 83, loss = 2965.39923580\n",
      "Iteration 84, loss = 2965.31011372\n",
      "Iteration 85, loss = 2965.40743712\n",
      "Iteration 86, loss = 2965.40979400\n",
      "Iteration 87, loss = 2965.40315184\n",
      "Iteration 88, loss = 2965.41039244\n",
      "Iteration 89, loss = 2965.40514261\n",
      "Iteration 90, loss = 2965.39424906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000004\n",
      "Iteration 91, loss = 2965.34286312\n",
      "Iteration 92, loss = 2965.35649470\n",
      "Iteration 93, loss = 2965.32722838\n",
      "Iteration 94, loss = 2965.32534191\n",
      "Iteration 95, loss = 2965.32173364\n",
      "Iteration 96, loss = 2965.31119120\n",
      "Iteration 97, loss = 2965.30419177\n",
      "Iteration 98, loss = 2965.30153698\n",
      "Iteration 99, loss = 2965.29471443\n",
      "Iteration 100, loss = 2965.30050258\n",
      "Iteration 101, loss = 2965.31051953\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 102, loss = 2965.33159184\n",
      "Iteration 103, loss = 2965.34834070\n",
      "Iteration 104, loss = 2965.34113653\n",
      "Iteration 105, loss = 2965.34134506\n",
      "Iteration 106, loss = 2965.34193322\n",
      "Iteration 107, loss = 2965.34744899\n",
      "Iteration 108, loss = 2965.34747098\n",
      "Iteration 109, loss = 2965.34765954\n",
      "Iteration 110, loss = 2965.34369118\n",
      "Iteration 111, loss = 2965.34384971\n",
      "Iteration 112, loss = 2965.34414312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic', batch_size=40, hidden_layer_sizes=(2, 3, 5),\n",
       "             learning_rate='adaptive', learning_rate_init=0.0001, max_iter=2000,\n",
       "             random_state=1, solver='sgd', verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg.fit(dataset.data,dataset.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.05968877,  0.18402967],\n",
       "        [-0.40630691, -0.16024792],\n",
       "        [-0.26285214, -0.31931863],\n",
       "        [-0.23692515, -0.11575231],\n",
       "        [-0.07524195,  0.0365495 ],\n",
       "        [-0.05835754,  0.15527665],\n",
       "        [-0.25788256,  0.29998264],\n",
       "        [-0.36742277,  0.1489553 ],\n",
       "        [-0.04410356,  0.06043797],\n",
       "        [-0.27663293, -0.23769095]]),\n",
       " array([[0.65616969, 1.12079108, 1.19076396],\n",
       "        [1.40563557, 0.37255791, 0.13276053]]),\n",
       " array([[1.40054   , 2.02778782, 1.78449417, 1.8850151 , 1.54941073],\n",
       "        [1.99212889, 2.22869645, 1.62207725, 2.3054102 , 2.54136512],\n",
       "        [1.40711191, 0.98781014, 1.62067409, 0.91124598, 1.25582593]]),\n",
       " array([[26.65084108],\n",
       "        [24.81273732],\n",
       "        [23.75916984],\n",
       "        [24.85838721],\n",
       "        [23.05584309]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coefs_\n",
    "\n",
    "\n",
    "#weigth matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.73381197, 0.57050669]),\n",
       " array([1.10385494, 1.98810897, 0.68652635]),\n",
       " array([2.98010473, 2.35516364, 2.56835809, 2.46743354, 2.11440652]),\n",
       " array([28.22758369])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 40,\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (2, 3, 5),\n",
       " 'learning_rate': 'adaptive',\n",
       " 'learning_rate_init': 0.0001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 2000,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1,\n",
       " 'shuffle': True,\n",
       " 'solver': 'sgd',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 2,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic', batch_size=40, hidden_layer_sizes=(2, 3, 5),\n",
       "             learning_rate='adaptive', learning_rate_init=0.0001, max_iter=2000,\n",
       "             random_state=1, solver='sgd', verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_test= [[ 0.03807591,.03807591,  0.05068012,  0.06169621,0.3807591 , -0.00259226,\n",
    "          0.01990842, -0.01764613, 0.3807591, 0.3807591],\n",
    "         \n",
    "         [ 0.03807591,.03807591,  0.05068012,  0.06169621,0.3807591 , -0.00259226,\n",
    "          0.01990842, -0.01764613, 0.3807591, 0.4444]\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [200,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.26557502, 151.26531565])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred=reg.predict(X_test)\n",
    "y_pred\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-141211080835.43826"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING KERAS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ON DIABETES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "dataset = load_diabetes()\n",
    "dataset.data\n",
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset\n",
    "\n",
    "x_train, y_train = dataset.data, dataset.target\n",
    "#x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# Set the input shape\n",
    "input_shape = (10,)\n",
    "\n",
    "print(f'Feature shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(16, input_shape=input_shape, activation='relu'))\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model \n",
    "\n",
    "\n",
    "model.compile(loss='mean_absolute_error', \n",
    "              optimizer='adam', \n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Desktop\\\\Rahul\\\\git\\\\DataSciencePython\\\\DeepLearning\\\\MLP'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Desktop\\Rahul\\git\\DataSciencePython\\DeepLearning\\MLP\\logs\\fit\\\n"
     ]
    }
   ],
   "source": [
    "# Define the Keras TensorBoard callback.\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "logdir=\"C:\\\\Users\\\\admin\\\\Desktop\\\\Rahul\\\\git\\\\DataSciencePython\\\\DeepLearning\\\\MLP\\\\logs\\\\fit\\\\\"\n",
    "print(logdir)\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 150.4788 - mean_squared_error: 28447.5957 - val_loss: 153.7303 - val_mean_squared_error: 30054.7871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2abd068bb20>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#and start training\n",
    "model.fit(x_train, y_train, \n",
    "          epochs=100, \n",
    "          batch_size=442, \n",
    "          verbose=1, \n",
    "          validation_split=0.2,\n",
    "          callbacks=[tensorboard_callback])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                176       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 321\n",
      "Trainable params: 321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION OF LAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tensorboard/graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 21740), started 0:27:05 ago. (Use '!kill 21740' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cc4c3fbef27987e4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cc4c3fbef27987e4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!tensorboard dev upload \\\n",
    "  --logdir logs \\\n",
    "  --name \"Sample op-level graph\" \\\n",
    "  --one_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ON CHENNAI WATER PROBLEM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = np.loadtxt('chennai_reservoir_levels.csv', delimiter='|', skiprows=1, usecols=(1,2,3,4))\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Shuffle dataset\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Separate features and targets\n",
    "X = dataset[:, 0:3]\n",
    "Y = dataset[:, 3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#follow the keras process as done for diabetes dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
